{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.A. Перцептрон и многослойный перцептрон (MLP).\n",
    "\n",
    "**Перцептрон (Perceptron):\n",
    "\n",
    "1.  **Входные данные (Inputs):** Перцептрон принимает на вход несколько числовых значений – признаки объекта, который мы хотим классифицировать.  Например, если мы хотим определить, является ли письмо спамом (0) или нет (1), признаками могут быть: количество определенных слов в письме, длина письма, наличие капслока и т.д.\n",
    "2.  **Веса (Weights) и Смещение (Bias):**  Каждый вход умножается на свой вес (`w1`, `w2` и т.д.). Веса показывают, насколько важен каждый вход.  Кроме того, добавляется еще один специальный вес – **смещение (bias)**, который обозначается обычно как `b`.\n",
    "3.  **Суммирование:**  Входы, умноженные на веса, и смещение суммируются.  Получается некая \"взвешенная сумма\": `z = w1*x1 + w2*x2 + b`\n",
    "4.  **Функция активации (Activation function):**  Полученная сумма `z` пропускается через **функцию активации**.  Функция активации решает, \"активируется\" ли нейрон (перцептрон) или нет.  Для простого перцептрона часто используют **ступенчатую функцию активации** (step function):\n",
    "\n",
    "    ```\n",
    "    activation(z) = 1, если z >= 0\n",
    "                   = 0, если z < 0\n",
    "    ```\n",
    "\n",
    "    Если взвешенная сумма `z` больше или равна нулю, перцептрон выдает `1` (например, \"не спам\"), иначе – `0` (\"спам\").\n",
    "\n",
    "Перцептрон, по сути, проводит **прямую линию** (в 2D) или **гиперплоскость** (в многомерном пространстве), которая разделяет эти классы.\n",
    "\n",
    "*   **Веса (`w1`, `w2`)** определяют наклон этой прямой.\n",
    "*   **Смещение (`b`)** сдвигает прямую, чтобы лучше отделить классы.\n",
    "\n",
    "Перцептрон реализует **линейную функцию**, за которой следует нелинейная ступенчатая функция.  Именно **линейная** часть (`w1*x1 + w2*x2 + b`) отвечает за разделяющую прямую.\n",
    "\n",
    "**Многослойный перцептрон (MLP) – решение проблемы нелинейности!**\n",
    "\n",
    "Чтобы решать более сложные, **нелинейные** задачи, мы можем объединить несколько перцептронов в **многослойную сеть**.  Это и есть **многослойный перцептрон (MLP)**, или, как его еще называют, **полносвязная нейронная сеть (Feedforward Neural Network)**.\n",
    "\n",
    "**Как устроен MLP:**\n",
    "\n",
    "1.  **Входной слой:**  Принимает входные данные (признаки).  Это как \"первый ряд\" перцептронов.\n",
    "2.  **Скрытые слои (Hidden Layers):**  Один или несколько слоев перцептронов, расположенных между входным и выходным слоями.  Именно **скрытые слои** позволяют MLP изучать сложные, нелинейные зависимости в данных.  Каждый нейрон в скрытом слое получает вход от **всех** нейронов предыдущего слоя (отсюда и \"полносвязная\").\n",
    "3.  **Выходной слой:**  Выдает результат работы сети.  Количество нейронов в выходном слое зависит от задачи (например, 1 нейрон для бинарной классификации, несколько нейронов для многоклассовой классификации).\n",
    "\n",
    "**Визуализация MLP:**\n",
    "\n",
    "```\n",
    "Входной слой   Скрытый слой 1   Скрытый слой 2   Выходной слой\n",
    "    O ---------> O ---------> O ---------> O\n",
    "    O ---------> O ---------> O ---------> O\n",
    "    O ---------> O ---------> O ---------> O\n",
    "```\n",
    "\n",
    "*   `O` – это нейрон (перцептрон).\n",
    "*   Стрелки показывают связи между нейронами (каждый нейрон в следующем слое связан со всеми нейронами в предыдущем слое).\n",
    "\n",
    "**Ключевая идея:**  Сочетание **множества перцептронов** и **скрытых слоев** позволяет MLP аппроксимировать (приближать) **любую сколь угодно сложную функцию**.  Именно в этом сила нейронных сетей!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.B. Функции активации: ReLU, Sigmoid, Tanh. *Роль **нелинейностей** в нейронных сетях*.\n",
    "\n",
    "**Функция активации** – это как \"выключатель\" для нейрона. Она определяет, должен ли нейрон \"активироваться\" (то есть выдавать сильный сигнал) или нет, в зависимости от взвешенной суммы его входов.\n",
    "\n",
    "**Функции активации вводят нелинейность в сеть.**  Это позволяет MLP моделировать сложные, нелинейные зависимости в данных и решать задачи, которые не под силу линейным моделям.\n",
    "\n",
    "**Cамые популярные функции активации:**\n",
    "\n",
    "1.  **Sigmoid (Сигмоида):**\n",
    "\n",
    "    *   **Формула:**  `sigmoid(z) = 1 / (1 + exp(-z))`\n",
    "    *   **Выходной диапазон:** от 0 до 1.\n",
    "    *   **Вид:** S-образная кривая.\n",
    "\n",
    "   Сигмоида \"сжимает\" любое входное значение в диапазон от 0 до 1.  Ее часто используют в **выходном слое** для задач **бинарной классификации**, где нужно получить вероятность принадлежности к классу (например, вероятность того, что письмо – спам).  Значение, близкое к 1, означает высокую вероятность, а значение, близкое к 0, – низкую.\n",
    "\n",
    "    **Плюсы:**  Выход в диапазоне [0, 1] интерпретируется как вероятность. Гладкая и дифференцируемая (важно для обучения, как увидим позже).\n",
    "\n",
    "    **Минусы:**  **Проблема затухания градиента (vanishing gradient problem)**.  При очень больших или очень маленьких значениях `z` сигмоида \"насыщается\" (становится очень пологой), и ее градиент (производная) становится очень маленьким.  Это замедляет или даже останавливает обучение сети, особенно в глубоких сетях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-7, 7, 100) # Создаем 100 точек от -7 до 7\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigmoid(z)\")\n",
    "plt.title(\"Сигмоидальная функция активации\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **Tanh (Гиперболический тангенс):**\n",
    "\n",
    "    *   **Формула:** `tanh(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))`\n",
    "    *   **Выходной диапазон:** от -1 до 1.\n",
    "    *   **Вид:**  Также S-образная кривая, но центрированная относительно нуля.\n",
    "\n",
    "    Tanh похож на сигмоиду, но его выходной диапазон от -1 до 1.  Иногда он сходится быстрее, чем сигмоида, из-за центрирования.  Также подвержен проблеме затухания градиента.\n",
    "\n",
    "    **Плюсы:**  Выход центрирован относительно нуля (может ускорить обучение). Дифференцируемая.\n",
    "\n",
    "    **Минусы:**  Также подвержена затуханию градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "plt.plot(z, tanh(z))\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"tanh(z)\")\n",
    "plt.title(\"Функция активации Tanh\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **ReLU (Rectified Linear Unit, Выпрямленная линейная функция):**\n",
    "\n",
    "    *   **Формула:** `ReLU(z) = max(0, z)`\n",
    "    *   **Выходной диапазон:** от 0 до +бесконечности.\n",
    "    *   **Вид:**  Линейна для положительных значений и равна 0 для отрицательных.\n",
    "\n",
    "    ReLU – очень простая, но при этом очень эффективная функция активации.  Если вход `z` положительный, ReLU пропускает его как есть.  Если `z` отрицательный или равен нулю, ReLU обнуляет его.\n",
    "\n",
    "    **Плюсы:**  **Решает проблему затухания градиента** для положительных значений `z` (градиент всегда равен 1).  Вычислительно очень эффективна (простое сравнение с нулем).  **Способствует разреженности представлений** (многие нейроны могут быть неактивны, что делает модель более эффективной и интерпретируемой).  Часто **сходится быстрее**, чем сигмоида и tanh.\n",
    "\n",
    "    **Минусы:**  **Проблема \"мертвых ReLU\" (dying ReLU problem)**.  Если нейрон получает постоянно отрицательный вход, то ReLU всегда будет выдавать 0, и градиент тоже будет 0.  Такой нейрон перестает обучаться и становится \"мертвым\".  Это может произойти, если learning rate слишком высокий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "plt.plot(z, relu(z))\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"ReLU(z)\")\n",
    "plt.title(\"Функция активации ReLU\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выбор функции активации:**\n",
    "\n",
    "*   **ReLU** – часто лучший выбор для **скрытых слоев** в большинстве задач. Она быстрая, эффективная и помогает избежать затухания градиента.\n",
    "*   **Sigmoid** – часто используют в **выходном слое** для задач **бинарной классификации**, когда нужен выход в виде вероятности [0, 1].\n",
    "*   **Tanh** – реже используется в скрытых слоях, но иногда может быть полезна, если нужно центрировать выход вокруг нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.C. Обратное распространение ошибки (Backpropagation):** *Детальное объяснение **цепного правила** и **градиентного спуска** для обучения многослойных сетей.\n",
    "\n",
    "Backpropagation – это алгоритм, который позволяет нам **эффективно обучать многослойные нейронные сети**.  Он основан на **градиентном спуске** и **цепном правиле** дифференцирования.  Звучит сложновато, но давай разберемся по шагам.\n",
    "\n",
    "**Вспомним, как вообще происходит обучение любой модели машинного обучения:**\n",
    "\n",
    "1.  **Прямой проход (Forward pass):** Мы подаем на вход модели данные (например, изображение кошки). Модель делает предсказание (например, \"это собака\").\n",
    "2.  **Вычисление ошибки (Loss calculation):** Мы сравниваем предсказание модели с правильным ответом (метка \"кошка\").  Разница между ними – это **ошибка (loss)**.  Чем больше ошибка, тем хуже работает модель.\n",
    "3.  **Обратное распространение ошибки (Backpropagation):**  Мы используем алгоритм backpropagation, чтобы **распространить ошибку обратно по сети** и **вычислить градиенты** – то есть узнать, как нужно изменить **веса** каждого нейрона, чтобы **уменьшить ошибку**.\n",
    "4.  **Обновление весов (Weight update):**  Используя **градиентный спуск** и вычисленные градиенты, мы **немного изменяем веса** в направлении уменьшения ошибки.\n",
    "5.  **Повторение шагов 1-4:** Мы повторяем эти шаги много раз, пока ошибка не станет достаточно маленькой, и модель не научится делать хорошие предсказания.\n",
    "\n",
    "**Backpropagation отвечает за самый важный шаг – шаг 3 (обратное распространение ошибки и вычисление градиентов).**\n",
    "\n",
    "**Давай разберемся, как это работает:**\n",
    "\n",
    "1.  **Цепное правило (Chain rule):**  Представь, что наша нейронная сеть – это сложная цепочка функций.  Чтобы понять, как изменение весов в *начале* цепочки влияет на ошибку в *конце* цепочки, нам нужно использовать **цепное правило** из математического анализа.\n",
    "\n",
    "    Цепное правило позволяет нам вычислять производную сложной функции как произведение производных ее составных частей.  В нашем случае \"составными частями\" являются слои нейронной сети и функции активации.\n",
    "\n",
    "2.  **Градиентный спуск (Gradient descent):**  Мы хотим **минимизировать ошибку (loss)**.  Представь, что ошибка – это высота горы, а веса нейронной сети – это координаты на поверхности земли.  Наша цель – спуститься с горы в самую низкую точку (минимум ошибки).\n",
    "\n",
    "    **Градиент** – это вектор, который указывает направление **наискорейшего подъема** на горе.  **Антиградиент** (градиент со знаком минус) указывает направление **наискорейшего спуска**.\n",
    "\n",
    "    **Градиентный спуск** – это метод оптимизации, который заключается в том, чтобы на каждом шаге двигаться в направлении **антиградиента**, чтобы постепенно спуститься к минимуму ошибки.\n",
    "\n",
    "**Как Backpropagation использует цепное правило и градиентный спуск:**\n",
    "\n",
    "*   Backpropagation **вычисляет градиент ошибки по отношению к каждому весу** в сети, используя **цепное правило**.  Он начинает с выходного слоя и двигается **обратно** к входному слою (отсюда и название \"обратное распространение\").\n",
    "*   На каждом слое backpropagation использует **локальные градиенты** (производные функций активации и линейных преобразований) и **умножает их**, чтобы получить градиент ошибки по отношению к весам этого слоя.\n",
    "*   После того, как градиенты вычислены для всех весов, мы используем **градиентный спуск**, чтобы **обновить веса** в направлении уменьшения ошибки.\n",
    "\n",
    "**Упрощенная аналогия:**\n",
    "\n",
    "Представь, что ты играешь в игру \"горячо-холодно\", чтобы найти клад, зарытый в холме.\n",
    "\n",
    "*   **Нейронная сеть** – это ты, пытающийся найти клад.\n",
    "*   **Веса сети** – это твои шаги по холму.\n",
    "*   **Ошибка (loss)** – это то, насколько далеко ты от клада.\n",
    "*   **Backpropagation** – это твой помощник, который говорит тебе: \"Сейчас ты идешь **слишком холодно**, поверни немного **вправо** (измени веса в определенном направлении), станет **горячее** (ошибка уменьшится)\".\n",
    "*   **Градиентный спуск** – это стратегия движения в направлении \"горячее\" (уменьшения ошибки), которую тебе подсказывает помощник.\n",
    "\n",
    "**Ключевые моменты Backpropagation:**\n",
    "\n",
    "*   **Эффективный способ вычисления градиентов** в многослойных сетях.\n",
    "*   Основан на **цепном правиле** и **градиентном спуске**.\n",
    "*   Позволяет **настраивать веса сети**, чтобы **минимизировать ошибку** и улучшать качество предсказаний.\n",
    "\n",
    "В следующих подпунктах (3.1 и 3.2) мы углубимся в математические детали backpropagation и рассмотрим проблемы, связанные с градиентами (vanishing/exploding gradients).  Но пока важно понять общую идею: **backpropagation – это \"сердце\" обучения нейронных сетей, алгоритм, который позволяет им учиться на данных.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VI.C. Обратное распространение ошибки (Backpropagation):** *Детальное объяснение **цепного правила** и **градиентного спуска** для обучения многослойных сетей*.\n",
    "\n",
    "Backpropagation – это алгоритм, который позволяет нам **эффективно обучать многослойные нейронные сети**.  Он основан на **градиентном спуске** и **цепном правиле** дифференцирования.  Звучит сложновато, но давай разберемся по шагам.\n",
    "\n",
    "**Вспомним, как вообще происходит обучение любой модели машинного обучения:**\n",
    "\n",
    "1.  **Прямой проход (Forward pass):** Мы подаем на вход модели данные (например, изображение кошки). Модель делает предсказание (например, \"это собака\").\n",
    "2.  **Вычисление ошибки (Loss calculation):** Мы сравниваем предсказание модели с правильным ответом (метка \"кошка\").  Разница между ними – это **ошибка (loss)**.  Чем больше ошибка, тем хуже работает модель.\n",
    "3.  **Обратное распространение ошибки (Backpropagation):**  Мы используем алгоритм backpropagation, чтобы **распространить ошибку обратно по сети** и **вычислить градиенты** – то есть узнать, как нужно изменить **веса** каждого нейрона, чтобы **уменьшить ошибку**.\n",
    "4.  **Обновление весов (Weight update):**  Используя **градиентный спуск** и вычисленные градиенты, мы **немного изменяем веса** в направлении уменьшения ошибки.\n",
    "5.  **Повторение шагов 1-4:** Мы повторяем эти шаги много раз, пока ошибка не станет достаточно маленькой, и модель не научится делать хорошие предсказания.\n",
    "\n",
    "**Backpropagation отвечает за самый важный шаг – шаг 3 (обратное распространение ошибки и вычисление градиентов).**\n",
    "\n",
    "**Давай разберемся, как это работает:**\n",
    "\n",
    "1.  **Цепное правило (Chain rule):**  Представь, что наша нейронная сеть – это сложная цепочка функций.  Чтобы понять, как изменение весов в *начале* цепочки влияет на ошибку в *конце* цепочки, нам нужно использовать **цепное правило** из математического анализа.\n",
    "\n",
    "    Цепное правило позволяет нам вычислять производную сложной функции как произведение производных ее составных частей.  В нашем случае \"составными частями\" являются слои нейронной сети и функции активации.\n",
    "\n",
    "2.  **Градиентный спуск (Gradient descent):**  Мы хотим **минимизировать ошибку (loss)**.  Представь, что ошибка – это высота горы, а веса нейронной сети – это координаты на поверхности земли.  Наша цель – спуститься с горы в самую низкую точку (минимум ошибки).\n",
    "\n",
    "    **Градиент** – это вектор, который указывает направление **наискорейшего подъема** на горе.  **Антиградиент** (градиент со знаком минус) указывает направление **наискорейшего спуска**.\n",
    "\n",
    "    **Градиентный спуск** – это метод оптимизации, который заключается в том, чтобы на каждом шаге двигаться в направлении **антиградиента**, чтобы постепенно спуститься к минимуму ошибки.\n",
    "\n",
    "**Как Backpropagation использует цепное правило и градиентный спуск:**\n",
    "\n",
    "*   Backpropagation **вычисляет градиент ошибки по отношению к каждому весу** в сети, используя **цепное правило**.  Он начинает с выходного слоя и двигается **обратно** к входному слою (отсюда и название \"обратное распространение\").\n",
    "*   На каждом слое backpropagation использует **локальные градиенты** (производные функций активации и линейных преобразований) и **умножает их**, чтобы получить градиент ошибки по отношению к весам этого слоя.\n",
    "*   После того, как градиенты вычислены для всех весов, мы используем **градиентный спуск**, чтобы **обновить веса** в направлении уменьшения ошибки.\n",
    "\n",
    "**Упрощенная аналогия:**\n",
    "\n",
    "Представь, что ты играешь в игру \"горячо-холодно\", чтобы найти клад, зарытый в холме.\n",
    "\n",
    "*   **Нейронная сеть** – это ты, пытающийся найти клад.\n",
    "*   **Веса сети** – это твои шаги по холму.\n",
    "*   **Ошибка (loss)** – это то, насколько далеко ты от клада.\n",
    "*   **Backpropagation** – это твой помощник, который говорит тебе: \"Сейчас ты идешь **слишком холодно**, поверни немного **вправо** (измени веса в определенном направлении), станет **горячее** (ошибка уменьшится)\".\n",
    "*   **Градиентный спуск** – это стратегия движения в направлении \"горячее\" (уменьшения ошибки), которую тебе подсказывает помощник.\n",
    "\n",
    "**Ключевые моменты Backpropagation:**\n",
    "\n",
    "*   **Эффективный способ вычисления градиентов** в многослойных сетях.\n",
    "*   Основан на **цепном правиле** и **градиентном спуске**.\n",
    "*   Позволяет **настраивать веса сети**, чтобы **минимизировать ошибку** и улучшать качество предсказаний.\n",
    "\n",
    "В следующих подпунктах (3.1 и 3.2) мы углубимся в математические детали backpropagation и рассмотрим проблемы, связанные с градиентами (vanishing/exploding gradients).  Но пока важно понять общую идею: **backpropagation – это \"сердце\" обучения нейронных сетей, алгоритм, который позволяет им учиться на данных.**\n",
    "\n",
    "**Вопросы для самопроверки:**\n",
    "\n",
    "1.  В чем заключается основная идея алгоритма обратного распространения ошибки (Backpropagation)?\n",
    "2.  Какие математические концепции лежат в основе Backpropagation?\n",
    "3.  Опиши простыми словами шаги алгоритма Backpropagation.\n",
    "4.  Зачем нужен градиентный спуск в обучении нейронных сетей?\n",
    "\n",
    "Готов углубиться в **математическое обоснование Backpropagation и chain rule**? Или есть вопросы по общей концепции?  Не стесняйся спрашивать! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VI.C. Обратное распространение ошибки (Backpropagation):** *Детальное объяснение **цепного правила** и **градиентного спуска** для обучения многослойных сетей*.\n",
    "\n",
    "Backpropagation – это алгоритм, который позволяет нам **эффективно обучать многослойные нейронные сети**.  Он основан на **градиентном спуске** и **цепном правиле** дифференцирования.  Звучит сложновато, но давай разберемся по шагам.\n",
    "\n",
    "**Вспомним, как вообще происходит обучение любой модели машинного обучения:**\n",
    "\n",
    "1.  **Прямой проход (Forward pass):** Мы подаем на вход модели данные (например, изображение кошки). Модель делает предсказание (например, \"это собака\").\n",
    "2.  **Вычисление ошибки (Loss calculation):** Мы сравниваем предсказание модели с правильным ответом (метка \"кошка\").  Разница между ними – это **ошибка (loss)**.  Чем больше ошибка, тем хуже работает модель.\n",
    "3.  **Обратное распространение ошибки (Backpropagation):**  Мы используем алгоритм backpropagation, чтобы **распространить ошибку обратно по сети** и **вычислить градиенты** – то есть узнать, как нужно изменить **веса** каждого нейрона, чтобы **уменьшить ошибку**.\n",
    "4.  **Обновление весов (Weight update):**  Используя **градиентный спуск** и вычисленные градиенты, мы **немного изменяем веса** в направлении уменьшения ошибки.\n",
    "5.  **Повторение шагов 1-4:** Мы повторяем эти шаги много раз, пока ошибка не станет достаточно маленькой, и модель не научится делать хорошие предсказания.\n",
    "\n",
    "**Backpropagation отвечает за самый важный шаг – шаг 3 (обратное распространение ошибки и вычисление градиентов).**\n",
    "\n",
    "**Давай разберемся, как это работает:**\n",
    "\n",
    "1.  **Цепное правило (Chain rule):**  Представь, что наша нейронная сеть – это сложная цепочка функций.  Чтобы понять, как изменение весов в *начале* цепочки влияет на ошибку в *конце* цепочки, нам нужно использовать **цепное правило** из математического анализа.\n",
    "\n",
    "    Цепное правило позволяет нам вычислять производную сложной функции как произведение производных ее составных частей.  В нашем случае \"составными частями\" являются слои нейронной сети и функции активации.\n",
    "\n",
    "2.  **Градиентный спуск (Gradient descent):**  Мы хотим **минимизировать ошибку (loss)**.  Представь, что ошибка – это высота горы, а веса нейронной сети – это координаты на поверхности земли.  Наша цель – спуститься с горы в самую низкую точку (минимум ошибки).\n",
    "\n",
    "    **Градиент** – это вектор, который указывает направление **наискорейшего подъема** на горе.  **Антиградиент** (градиент со знаком минус) указывает направление **наискорейшего спуска**.\n",
    "\n",
    "    **Градиентный спуск** – это метод оптимизации, который заключается в том, чтобы на каждом шаге двигаться в направлении **антиградиента**, чтобы постепенно спуститься к минимуму ошибки.\n",
    "\n",
    "**Как Backpropagation использует цепное правило и градиентный спуск:**\n",
    "\n",
    "*   Backpropagation **вычисляет градиент ошибки по отношению к каждому весу** в сети, используя **цепное правило**.  Он начинает с выходного слоя и двигается **обратно** к входному слою (отсюда и название \"обратное распространение\").\n",
    "*   На каждом слое backpropagation использует **локальные градиенты** (производные функций активации и линейных преобразований) и **умножает их**, чтобы получить градиент ошибки по отношению к весам этого слоя.\n",
    "*   После того, как градиенты вычислены для всех весов, мы используем **градиентный спуск**, чтобы **обновить веса** в направлении уменьшения ошибки.\n",
    "\n",
    "**Упрощенная аналогия:**\n",
    "\n",
    "Представь, что ты играешь в игру \"горячо-холодно\", чтобы найти клад, зарытый в холме.\n",
    "\n",
    "*   **Нейронная сеть** – это ты, пытающийся найти клад.\n",
    "*   **Веса сети** – это твои шаги по холму.\n",
    "*   **Ошибка (loss)** – это то, насколько далеко ты от клада.\n",
    "*   **Backpropagation** – это твой помощник, который говорит тебе: \"Сейчас ты идешь **слишком холодно**, поверни немного **вправо** (измени веса в определенном направлении), станет **горячее** (ошибка уменьшится)\".\n",
    "*   **Градиентный спуск** – это стратегия движения в направлении \"горячее\" (уменьшения ошибки), которую тебе подсказывает помощник.\n",
    "\n",
    "**Ключевые моменты Backpropagation:**\n",
    "\n",
    "*   **Эффективный способ вычисления градиентов** в многослойных сетях.\n",
    "*   Основан на **цепном правиле** и **градиентном спуске**.\n",
    "*   Позволяет **настраивать веса сети**, чтобы **минимизировать ошибку** и улучшать качество предсказаний.\n",
    "\n",
    "В следующих подпунктах (3.1 и 3.2) мы углубимся в математические детали backpropagation и рассмотрим проблемы, связанные с градиентами (vanishing/exploding gradients).  Но пока важно понять общую идею: **backpropagation – это \"сердце\" обучения нейронных сетей, алгоритм, который позволяет им учиться на данных.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Тогда погружаемся в математические дебри Backpropagation. 🚀 Не пугайся формул, я постараюсь объяснить максимально понятно и интуитивно. Главное – понять суть, а формулы – это просто инструмент для точного описания процесса.\n",
    "\n",
    "### 3.1. **Математическое обоснование алгоритма обратного распространения ошибки, chain rule в деталях 📝.**\n",
    "\n",
    "**Цель Backpropagation:**  Вычислить **градиенты** функции потерь (ошибки) по **всем весам** нейронной сети.  Зачем нам это? Чтобы с помощью градиентного спуска обновить веса и уменьшить ошибку.\n",
    "\n",
    "**Вспоминаем цепное правило (chain rule):**\n",
    "\n",
    "Если у нас есть сложная функция, например, `f(x) = g(h(x))`, то производная `f'(x)` вычисляется как:\n",
    "\n",
    "`f'(x) = g'(h(x)) * h'(x)`\n",
    "\n",
    "То есть, производная внешней функции `g`, вычисленная в точке `h(x)`, умножается на производную внутренней функции `h`.  Цепное правило позволяет нам \"разложить\" производную сложной функции на произведение производных более простых функций.\n",
    "\n",
    "**Как это применяется к нейронной сети?**\n",
    "\n",
    "Представь себе простую двухслойную нейронную сеть (для простоты объяснения).\n",
    "\n",
    "*   **Входной слой:**  `x` (вектор входов)\n",
    "*   **Скрытый слой:**\n",
    "    *   Линейное преобразование: `z = W1 * x + b1`  (где `W1` – матрица весов первого слоя, `b1` – вектор смещений первого слоя)\n",
    "    *   Функция активации: `a = activation(z)` (например, ReLU)\n",
    "*   **Выходной слой:**\n",
    "    *   Линейное преобразование: `o = W2 * a + b2` (где `W2` – матрица весов второго слоя, `b2` – вектор смещений второго слоя)\n",
    "    *   Функция активации (или просто выход): `ŷ = output_activation(o)` (например, Sigmoid для бинарной классификации или Softmax для многоклассовой)\n",
    "\n",
    "**Функция потерь (Loss function):**  `L(ŷ, y)` – измеряет разницу между предсказанным значением `ŷ` и истинным значением `y`.  Например, Mean Squared Error (MSE) для регрессии или Cross-Entropy Loss для классификации.\n",
    "\n",
    "**Наша задача:**  Найти градиенты функции потерь `L` по отношению ко **всем весам** (`W1`, `W2`) и **смещениям** (`b1`, `b2`).  Например, нам нужно вычислить:\n",
    "\n",
    "*   `∂L / ∂W2`\n",
    "*   `∂L / ∂b2`\n",
    "*   `∂L / ∂W1`\n",
    "*   `∂L / ∂b1`\n",
    "\n",
    "**Обратное распространение ошибки – пошагово:**\n",
    "\n",
    "1.  **Прямой проход (Forward pass):** Сначала мы делаем прямой проход, вычисляя значения `z`, `a`, `o`, `ŷ` для заданного входа `x`.  Это мы уже умеем.\n",
    "\n",
    "2.  **Вычисление ошибки на выходе (Output layer error):**  Начинаем с **выходного слоя**.  Нам нужно вычислить, как ошибка `L` меняется при изменении выхода *линейного преобразования* выходного слоя `o`.  То есть, нам нужен градиент:\n",
    "\n",
    "    `∂L / ∂o`\n",
    "\n",
    "    Это можно вычислить, используя производную функции потерь `L` и производную выходной функции активации `output_activation` (если она есть).  Например, если у нас MSE и линейный выход (без активации на выходе), то `∂L / ∂o` будет связано с разницей между `ŷ` и `y`.  Конкретный вид зависит от выбранной функции потерь.\n",
    "\n",
    "3.  **Обратное распространение ошибки к весам выходного слоя (Output layer weights gradient):** Теперь, когда у нас есть `∂L / ∂o`, мы можем вычислить градиент ошибки по отношению к **весам выходного слоя** `W2` и **смещению** `b2`.  Используем цепное правило:\n",
    "\n",
    "    *   `∂L / ∂W2 = (∂L / ∂o) * (∂o / ∂W2)`\n",
    "    *   `∂L / ∂b2 = (∂L / ∂o) * (∂o / ∂b2)`\n",
    "\n",
    "    Здесь:\n",
    "    *   `(∂o / ∂W2)` – это производная `o = W2 * a + b2` по `W2`.  Она равна просто `a` (выходу предыдущего слоя).\n",
    "    *   `(∂o / ∂b2)` – это производная `o = W2 * a + b2` по `b2`. Она равна просто `1`.\n",
    "\n",
    "    **Интуиция:**  Чтобы изменить ошибку `L`, изменяя веса `W2`, нам нужно знать, как `W2` влияет на `o` (через `∂o / ∂W2 = a`) и как `o` влияет на `L` (через `∂L / ∂o`).  Цепное правило \"связывает\" эти влияния.\n",
    "\n",
    "4.  **Обратное распространение ошибки к активациям скрытого слоя (Hidden layer activations error):**  Теперь нам нужно \"пробросить\" ошибку **дальше назад**, к **скрытому слою**.  Нам нужно понять, как ошибка `L` меняется при изменении **выхода функции активации скрытого слоя** `a`.  То есть, нам нужен градиент:\n",
    "\n",
    "    `∂L / ∂a`\n",
    "\n",
    "    И снова используем цепное правило:\n",
    "\n",
    "    `∂L / ∂a = (∂L / ∂o) * (∂o / ∂a)`\n",
    "\n",
    "    Здесь:\n",
    "    *   `(∂o / ∂a)` – это производная `o = W2 * a + b2` по `a`. Она равна просто `W2` (матрице весов выходного слоя).  *Важно: здесь получается матрица, так как `W2` – это матрица, а `a` – вектор.*\n",
    "\n",
    "    **Интуиция:**  Чтобы понять, как изменение активаций `a` влияет на ошибку `L`, мы смотрим, как `a` влияет на выход `o` (через `∂o / ∂a = W2`) и как `o` влияет на `L` (через `∂L / ∂o`).\n",
    "\n",
    "5.  **Обратное распространение ошибки к линейному преобразованию скрытого слоя (Hidden layer linear transformation error):** Теперь у нас есть `∂L / ∂a`.  Нам нужно \"пройти\" через **функцию активации скрытого слоя** `activation`.  Нам нужно вычислить, как ошибка `L` меняется при изменении **выхода линейного преобразования скрытого слоя** `z`.  То есть, нам нужен градиент:\n",
    "\n",
    "    `∂L / ∂z`\n",
    "\n",
    "    Используем цепное правило:\n",
    "\n",
    "    `∂L / ∂z = (∂L / ∂a) * (∂a / ∂z)`\n",
    "\n",
    "    Здесь:\n",
    "    *   `(∂a / ∂z)` – это производная **функции активации** `a = activation(z)` по `z`.  Например, если `activation` – ReLU, то производная будет 1, если `z > 0`, и 0, если `z <= 0`.  Производная сигмоиды и tanh тоже известны и легко вычисляются.\n",
    "\n",
    "    **Интуиция:**  Чтобы понять, как изменение `z` влияет на ошибку `L`, мы смотрим, как `z` влияет на `a` (через `∂a / ∂z` – производную функции активации) и как `a` влияет на `L` (через `∂L / ∂a`).\n",
    "\n",
    "6.  **Обратное распространение ошибки к весам скрытого слоя (Hidden layer weights gradient):**  И наконец, у нас есть `∂L / ∂z`.  Мы можем вычислить градиент ошибки по отношению к **весам скрытого слоя** `W1` и **смещению** `b1`.  Используем цепное правило:\n",
    "\n",
    "    *   `∂L / ∂W1 = (∂L / ∂z) * (∂z / ∂W1)`\n",
    "    *   `∂L / ∂b1 = (∂L / ∂z) * (∂z / ∂b1)`\n",
    "\n",
    "    Здесь:\n",
    "    *   `(∂z / ∂W1)` – это производная `z = W1 * x + b1` по `W1`. Она равна просто `x` (входу сети).\n",
    "    *   `(∂z / ∂b1)` – это производная `z = W1 * x + b1` по `b1`. Она равна просто `1`.\n",
    "\n",
    "    **Интуиция:**  Аналогично выходному слою, чтобы изменить ошибку `L`, изменяя веса `W1`, нам нужно знать, как `W1` влияет на `z` (через `∂z / ∂W1 = x`) и как `z` влияет на `L` (через `∂L / ∂z`).\n",
    "\n",
    "7.  **Обновление весов (Weight update):**  После того, как мы вычислили все градиенты (`∂L / ∂W2`, `∂L / ∂b2`, `∂L / ∂W1`, `∂L / ∂b1`), мы используем **градиентный спуск** для обновления весов и смещений:\n",
    "\n",
    "    *   `W2 = W2 - learning_rate * (∂L / ∂W2)`\n",
    "    *   `b2 = b2 - learning_rate * (∂L / ∂b2)`\n",
    "    *   `W1 = W1 - learning_rate * (∂L / ∂W1)`\n",
    "    *   `b1 = b1 - learning_rate * (∂L / ∂b1)`\n",
    "\n",
    "    где `learning_rate` – это **скорость обучения**, маленький положительный параметр, который контролирует размер шага градиентного спуска.\n",
    "\n",
    "**Ключевые моменты математического обоснования Backpropagation:**\n",
    "\n",
    "*   **Цепное правило – основа всего алгоритма.**  Оно позволяет \"разложить\" градиент сложной функции потерь на произведение локальных градиентов.\n",
    "*   **Движение \"назад\" по сети.**  Градиенты вычисляются, начиная с выходного слоя и распространяясь обратно к входному слою.\n",
    "*   **Локальные градиенты.**  На каждом шаге мы вычисляем простые производные (производные линейных преобразований и функций активации).\n",
    "*   **Эффективность.**  Backpropagation позволяет эффективно вычислять градиенты для **всех весов** сети за один проход.  Без него обучение глубоких сетей было бы вычислительно невозможным.\n",
    "\n",
    "**Важно:**  Конкретные формулы для градиентов зависят от выбора **функции потерь** и **функций активации**.  Но общая схема алгоритма Backpropagation остается той же.\n",
    "\n",
    "**Упрощенная аналогия с заводом:**\n",
    "\n",
    "Представь, что нейронная сеть – это завод, производящий продукцию (предсказания).\n",
    "\n",
    "*   **Веса и смещения** – это настройки станков на заводе.\n",
    "*   **Ошибка (loss)** – это количество бракованной продукции.\n",
    "*   **Backpropagation** – это система контроля качества, которая отслеживает, на каком этапе производства возникает брак, и дает рекомендации, как подкрутить настройки станков (веса), чтобы уменьшить брак (ошибку).\n",
    "*   **Цепное правило** – это как \"связи\" между цехами завода.  Если брак возникает в конце производственной линии, система контроля качества \"прослеживает\" его обратно по цепочке, чтобы найти, какой станок на каком этапе виноват, и как его настроить.\n",
    "\n",
    "**Вопросы для самопроверки:**\n",
    "\n",
    "1.  Какова цель алгоритма Backpropagation с математической точки зрения?\n",
    "2.  Какую роль играет цепное правило в Backpropagation? Объясни своими словами.\n",
    "3.  Почему Backpropagation называется \"обратным распространением\"? В каком направлении распространяется ошибка?\n",
    "4.  Опиши основные шаги алгоритма Backpropagation, опираясь на математические формулы (хотя бы на словах, если формулы пока сложно запомнить).\n",
    "\n",
    "Готов двигаться дальше и рассмотреть проблему **vanishing/exploding gradients** и способы ее решения?  Или есть вопросы по математическому обоснованию Backpropagation?  Не стесняйся спрашивать! 😊  Это может быть непросто, но очень важно для понимания глубокого обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Молодец! 👍 Очень хорошие ответы, показывают понимание ключевых моментов. Давай немного уточним и закрепим:\n",
    "\n",
    "1.  **Нахождение производных от ошибки до входного слоя** -  Верно, но лучше сказать **\"Нахождение производных функции потерь (ошибки) по *всем* весам нейронной сети\"**.  Backpropagation вычисляет, как изменение *каждого* веса влияет на общую ошибку, и делает это для всех весов, от выходного слоя до входного.\n",
    "\n",
    "2.  **Нахождение того, какой конкретно вес нужно регулировать для уменьшения ошибки** -  Абсолютно точно! Именно для этого и нужны градиенты. Градиент показывает направление *наискорейшего роста* ошибки.  А **антиградиент** (градиент со знаком минус) показывает направление *наискорейшего уменьшения* ошибки.  Backpropagation говорит нам, **в какую сторону и насколько нужно изменить каждый вес**, чтобы ошибка стала меньше.\n",
    "\n",
    "3.  **От выхода до входа** -  Да, именно так! **\"Обратное\"** распространение означает, что ошибка и градиенты распространяются **в обратном направлении** от прямого прохода – от выходного слоя к входному.\n",
    "\n",
    "4.  **нахождение производных шаг за шагом** -  Да, это **пошаговый процесс** вычисления производных.  И ключевое здесь – **цепное правило**. Backpropagation, по сути, является эффективным способом применения цепного правила для вычисления градиентов в сложных многослойных функциях, которыми и являются нейронные сети.  Мы как бы \"разматываем\" сложную функцию на более простые, и шаг за шагом вычисляем производные.\n",
    "\n",
    "**Краткое резюме Backpropagation:**\n",
    "\n",
    "*   **Цель:** Эффективно вычислить градиенты функции потерь по *всем* весам нейронной сети.\n",
    "*   **Механизм:**  Использует **цепное правило** для пошагового вычисления градиентов, распространяя ошибку **в обратном направлении** от выходного слоя к входному.\n",
    "*   **Результат:** Градиенты показывают, как нужно изменить каждый вес, чтобы **минимизировать ошибку**.\n",
    "*   **Применение:**  Градиенты используются в алгоритме **градиентного спуска** для обновления весов и обучения нейронной сети.\n",
    "\n",
    "**Отлично!** Ты хорошо понял основные принципы Backpropagation.  Теперь мы можем перейти к очень важной проблеме, которая возникает при обучении глубоких нейронных сетей – **проблеме затухающих и взрывающихся градиентов (vanishing/exploding gradients)**.  И, конечно, обсудим способы борьбы с ней.\n",
    "\n",
    "### 3.2. **Vanishing/exploding gradients problems и способы их решения 💥.**\n",
    "\n",
    "**Проблема затухающих градиентов (Vanishing Gradients):**\n",
    "\n",
    "Представь, что ты живешь на верхнем этаже небоскреба и хочешь спуститься на первый этаж по лестнице в полной темноте.  Градиентный спуск – это как твои шаги по лестнице.  Если градиенты (твои шаги) становятся **очень маленькими**, то ты будешь двигаться к цели (первому этажу) **очень медленно**, практически не двигаясь.  В худшем случае ты можешь вообще **застрять** на каком-то этаже и не дойти до конца.\n",
    "\n",
    "В нейронных сетях **затухающие градиенты** возникают, когда градиенты становятся **экспоненциально малыми** по мере распространения ошибки **обратно** через слои сети.  Особенно сильно это проявляется в **глубоких нейронных сетях** (с большим количеством слоев).\n",
    "\n",
    "**Почему возникают затухающие градиенты?**\n",
    "\n",
    "*   **Сигмоида и Tanh:**  Классические функции активации, такие как сигмоида и tanh, имеют производные, которые **всегда меньше 1** (максимум 0.25 для сигмоиды и 1 для tanh, но в основном меньше).  Когда мы используем цепное правило для backpropagation, мы **умножаем эти производные** на каждом слое.  Если производных много, и все они меньше 1, то их произведение становится **очень маленьким числом** (стремится к нулю экспоненциально с глубиной сети).\n",
    "*   **Умножение малых чисел:**  Представь, что мы умножаем много чисел, например, 0.5 * 0.5 * 0.5 * ... * 0.5 (много раз).  Результат очень быстро становится крошечным.  То же самое происходит с градиентами при backpropagation, если производные функций активации малы.\n",
    "\n",
    "**Последствия затухающих градиентов:**\n",
    "\n",
    "*   **Медленное обучение:**  Веса **в ранних слоях** (ближе к входу) практически **не обновляются**, так как градиенты для них очень малы.  Сеть \"забывает\" про входные данные и не может выучить полезные признаки из них.\n",
    "*   **Застревание в локальных минимумах:**  Обучение может застрять в **неоптимальном локальном минимуме** функции потерь, так как градиенты слишком малы, чтобы \"вытолкнуть\" сеть из этого минимума.\n",
    "*   **Невозможность обучения глубоких сетей:**  В особо тяжелых случаях глубокие сети с затухающими градиентами **фактически не обучаются**.  Они работают не лучше, чем случайные веса.\n",
    "\n",
    "**Проблема взрывающихся градиентов (Exploding Gradients):**\n",
    "\n",
    "Это противоположная проблема затухающим градиентам.  **Взрывающиеся градиенты** возникают, когда градиенты становятся **экспоненциально большими** по мере распространения ошибки обратно через слои сети.\n",
    "\n",
    "**Почему возникают взрывающиеся градиенты?**\n",
    "\n",
    "*   **Большие веса:**  Если веса в сети становятся **большими**, то при backpropagation градиенты могут **умножаться на большие числа** на каждом слое.  Это может привести к тому, что градиенты будут **экспоненциально расти** с глубиной сети.\n",
    "*   **Производные больше 1:**  Хотя сигмоида и tanh имеют производные меньше 1, другие функции активации (например, ReLU в своей линейной части) имеют производную, равную 1.  Если веса достаточно большие, и производные функций активации не \"гасят\" градиенты, то они могут взрываться.\n",
    "\n",
    "**Последствия взрывающихся градиентов:**\n",
    "\n",
    "*   **Нестабильность обучения:**  Веса могут **резко меняться** на каждом шаге обучения, что приводит к **нестабильности** и **колебаниям** функции потерь.\n",
    "*   **Расходимость обучения:**  В худшем случае обучение может **расходиться**, то есть функция потерь начинает **расти**, а не уменьшаться.\n",
    "*   **NaN значения:**  Градиенты могут стать настолько большими, что вычисления приводят к **ошибкам переполнения** и появлению значений `NaN` (Not a Number).\n",
    "\n",
    "**Способы решения проблем vanishing/exploding gradients:**\n",
    "\n",
    "1.  **Функции активации:**\n",
    "    *   **ReLU и ее вариации (Leaky ReLU, ELU, GELU и т.д.):**  ReLU (и ее вариации) помогают **бороться с затухающими градиентами**, так как их производная равна 1 для положительных входов (не \"гасит\" градиент).  Они стали очень популярными именно из-за этого.  Leaky ReLU и ELU немного смягчают проблему \"мертвых ReLU\".\n",
    "    *   **Избегать Sigmoid и Tanh в глубоких сетях (в скрытых слоях):**  Для глубоких сетей лучше использовать ReLU или ее вариации в скрытых слоях, а Sigmoid и Tanh оставить для выходных слоев (если это необходимо для конкретной задачи).\n",
    "\n",
    "2.  **Инициализация весов (Weight Initialization):**\n",
    "    *   **Правильная инициализация весов** может помочь предотвратить как затухающие, так и взрывающиеся градиенты на **начальных этапах обучения**.  Мы рассмотрим методы инициализации весов подробнее в пункте VI.G (Xavier/Glorot, He initialization).  Идея в том, чтобы **инициализировать веса не слишком большими и не слишком маленькими**, а из определенного **оптимального диапазона**.\n",
    "\n",
    "3.  **BatchNorm (Batch Normalization):**\n",
    "    *   **Batch Normalization** – это техника **нормализации** выходов слоев в процессе обучения.  Она помогает **стабилизировать обучение**, **ускорить сходимость** и **уменьшить влияние vanishing/exploding gradients**.  BatchNorm нормализует выходы каждого слоя, приводя их к **нулевому среднему и единичной дисперсии** в каждом мини-батче.  Это помогает градиентам оставаться в \"здоровом\" диапазоне.  BatchNorm также действует как форма **регуляризации**.  Мы рассмотрим BatchNorm подробнее в пункте VI.E.\n",
    "\n",
    "4.  **Gradient Clipping (Ограничение градиентов):**\n",
    "    *   **Gradient Clipping** – это техника, которая помогает бороться с **взрывающимися градиентами**.  Она заключается в **ограничении значений градиентов** сверху и снизу некоторым пороговым значением.  Если градиент становится слишком большим, его \"обрезают\" до порогового значения.  Это предотвращает \"взрыв\" градиентов и делает обучение более стабильным.\n",
    "\n",
    "5.  **Архитектура сети:**\n",
    "    *   Иногда проблема vanishing/exploding gradients может быть связана с **архитектурой сети**.  Например, использование **рекуррентных нейронных сетей (RNN)** без специальных механизмов (таких как LSTM или GRU) часто приводит к проблемам с градиентами.  Выбор подходящей архитектуры для задачи также важен.\n",
    "\n",
    "**Подведем итог:**\n",
    "\n",
    "*   **Vanishing gradients** – градиенты становятся слишком малыми, обучение замедляется или останавливается, особенно в ранних слоях глубоких сетей.  Связано с функциями активации типа Sigmoid и Tanh и умножением малых производных.\n",
    "*   **Exploding gradients** – градиенты становятся слишком большими, обучение нестабильно, может расходиться.  Связано с большими весами и умножением больших градиентов.\n",
    "*   **Решения:**  ReLU и ее вариации, правильная инициализация весов, BatchNorm, Gradient Clipping, выбор подходящей архитектуры.\n",
    "\n",
    "**Вопросы для самопроверки:**\n",
    "\n",
    "1.  В чем суть проблемы затухающих градиентов? Почему она возникает?\n",
    "2.  В чем суть проблемы взрывающихся градиентов? Почему она возникает?\n",
    "3.  Какие функции активации более склонны к затуханию градиентов? Какие – менее склонны?\n",
    "4.  Какие способы решения проблем vanishing/exploding gradients ты знаешь? Опиши хотя бы 2-3 способа.\n",
    "\n",
    "Готов двигаться дальше и рассмотреть **градиентный спуск и оптимизаторы** (Adam, SGD)?  Или есть вопросы по vanishing/exploding gradients?  Эта тема важна для понимания ограничений и возможностей глубокого обучения, так что не стесняйся спрашивать, если что-то неясно! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По плану так по плану!  👍  Отлично, переходим к пункту **VI.E. BatchNorm и Dropout**.  Это две очень мощные и часто используемые техники в нейронных сетях.  Они служат двум главным целям:\n",
    "\n",
    "1.  **Регуляризация (Regularization):**  Помогают предотвратить **переобучение (overfitting)**, то есть ситуацию, когда модель отлично работает на обучающих данных, но плохо обобщается на новые, \"невиданные\" данные.\n",
    "2.  **Ускорение обучения (Accelerating Training):**  Помогают нейронным сетям **учиться быстрее** и **сходиться к лучшему решению**.\n",
    "\n",
    "Разберем каждую технику по порядку.\n",
    "\n",
    "### VI.E. **BatchNorm и Dropout:** *Техники **регуляризации** и **ускорения обучения** в нейронных сетях*.\n",
    "\n",
    "**1. BatchNorm (Batch Normalization) – Пакетная Нормализация:**\n",
    "\n",
    "**Что такое BatchNorm?**\n",
    "\n",
    "**Batch Normalization (BatchNorm)** – это техника, которая **нормализует выходы слоев нейронной сети в каждом мини-батче** во время обучения.  Звучит сложно, но давай разберемся по шагам.\n",
    "\n",
    "**Вспоминаем нормализацию данных:**  Когда мы готовим данные для обучения модели, мы часто нормализуем входные признаки (например, стандартизируем, приводя к нулевому среднему и единичной дисперсии).  Это помогает моделям учиться быстрее и эффективнее.\n",
    "\n",
    "**Идея BatchNorm:**  А что, если мы будем нормализовывать **не только входные данные, но и выходы *каждого* слоя** нейронной сети?  Именно это и делает BatchNorm!\n",
    "\n",
    "**Как работает BatchNorm:**\n",
    "\n",
    "Для каждого слоя (обычно после линейного слоя и перед функцией активации) BatchNorm делает следующее для каждого мини-батча:\n",
    "\n",
    "1.  **Вычисляет среднее значение (mean) и дисперсию (variance) выходов слоя в текущем мини-батче.**\n",
    "2.  **Нормализует выходы слоя:**  вычитает среднее и делит на стандартное отклонение (квадратный корень из дисперсии).  После этой операции выходы слоя будут иметь **нулевое среднее и единичную дисперсию** (внутри мини-батча).\n",
    "3.  **Масштабирование и сдвиг:**  Добавляет **два learnable параметра** для каждого выхода слоя: **масштаб (gamma, γ)** и **сдвиг (beta, β)**.  Нормализованные значения умножаются на γ и к ним прибавляется β.\n",
    "\n",
    "    Формулы BatchNorm (упрощенно):\n",
    "\n",
    "    ```\n",
    "    μ_B = (1/m) * Σ(x_i)             // Среднее значение в мини-батче B\n",
    "    σ^2_B = (1/m) * Σ((x_i - μ_B)^2)  // Дисперсия в мини-батче B\n",
    "    x_hat_i = (x_i - μ_B) / √(σ^2_B + ε) // Нормализация\n",
    "    y_i = γ * x_hat_i + β            // Масштабирование и сдвиг\n",
    "    ```\n",
    "\n",
    "    где:\n",
    "    *   `x_i` – выходы слоя в мини-батче.\n",
    "    *   `μ_B`, `σ^2_B` – среднее и дисперсия мини-батча.\n",
    "    *   `x_hat_i` – нормализованные значения.\n",
    "    *   `γ`, `β` – learnable параметры (обучаемые веса).\n",
    "    *   `ε` – маленькое число для предотвращения деления на ноль.\n",
    "    *   `y_i` – выходы BatchNorm слоя.\n",
    "\n",
    "**Зачем нужен BatchNorm?  Преимущества BatchNorm:**\n",
    "\n",
    "*   **Ускорение обучения (Accelerated Learning):**\n",
    "    *   **Улучшает распространение градиентов:**  BatchNorm делает функцию потерь более гладкой и \"легкой\" для оптимизации.  Градиенты становятся более \"здоровыми\" (не такими маленькими и не такими большими), что **ускоряет сходимость**.\n",
    "    *   **Позволяет использовать более высокие learning rates:**  Из-за стабилизации градиентов можно использовать более высокие learning rates, не опасаясь нестабильности обучения.\n",
    "    *   **Уменьшает внутреннее смещение ковариат (Internal Covariate Shift):**  BatchNorm уменьшает изменения в распределении входных данных для каждого слоя в процессе обучения.  Это стабилизирует обучение и позволяет слоям учиться более независимо друг от друга.\n",
    "\n",
    "*   **Регуляризация (Regularization):**\n",
    "    *   **Снижает зависимость от инициализации весов:**  BatchNorm делает обучение менее чувствительным к выбору начальных весов.\n",
    "    *   **Небольшой эффект регуляризации:**  За счет нормализации в мини-батчах BatchNorm вносит некоторый \"шум\" в процесс обучения, что может немного **уменьшить переобучение**.  Хотя это и не основная цель BatchNorm, но это приятный побочный эффект.\n",
    "\n",
    "*   **Снижение проблемы vanishing/exploding gradients:**  Нормализация выходов слоев помогает **ограничить значения градиентов** и **уменьшить проблемы затухания и взрыва градиентов**, особенно в глубоких сетях.\n",
    "\n",
    "**Где использовать BatchNorm?**\n",
    "\n",
    "BatchNorm обычно добавляют **после линейных (или сверточных) слоев и перед функциями активации**.  Например:\n",
    "\n",
    "`Linear -> BatchNorm -> Activation -> ...`\n",
    "\n",
    "или\n",
    "\n",
    "`Conv2D -> BatchNorm -> Activation -> ...`\n",
    "\n",
    "**Важно:  Различие между обучением (training) и применением (inference) BatchNorm:**\n",
    "\n",
    "*   **Обучение (Training):**  Во время обучения BatchNorm использует **среднее и дисперсию *текущего мини-батча*** для нормализации.\n",
    "*   **Применение (Inference/Testing):**  Во время применения (когда мы используем обученную сеть для предсказаний на новых данных), мы **не можем** вычислять среднее и дисперсию для каждого отдельного примера.  Вместо этого BatchNorm использует **скользящие средние** (running averages) среднего и дисперсии, вычисленные **на *всех* мини-батчах во время обучения**.  Эти скользящие средние аппроксимируют среднее и дисперсию **всего обучающего набора данных**.\n",
    "\n",
    "**2. Dropout – Выпадение:**\n",
    "\n",
    "**Что такое Dropout?**\n",
    "\n",
    "**Dropout** – это простая, но очень эффективная техника **регуляризации** для нейронных сетей.  Идея Dropout заключается в том, чтобы **случайно \"выключать\" (обнулять) некоторые нейроны** в сети во время обучения с определенной вероятностью.\n",
    "\n",
    "**Как работает Dropout?**\n",
    "\n",
    "Во время **прямого прохода** в процессе обучения для каждого мини-батча:\n",
    "\n",
    "1.  Для каждого слоя, к которому применяется Dropout, **случайно выбирается некоторое количество нейронов** (обычно задается **вероятность dropout, например, 0.5**).\n",
    "2.  Выбранные нейроны временно **\"выключаются\"**, то есть их выходы **обнуляются**.  Остальные нейроны работают как обычно.\n",
    "3.  Обратное распространение ошибки (backpropagation) происходит только через **\"активные\" нейроны** (те, которые не были выключены).\n",
    "\n",
    "Во время **применения (inference/testing)** Dropout **не используется**.  Все нейроны работают как обычно.\n",
    "\n",
    "**Вероятность Dropout (Dropout rate, p):**  Это гиперпараметр, который задает вероятность \"выключения\" нейрона.  Обычно выбирают значения в диапазоне **0.2 - 0.5**.  Более высокие значения dropout rate сильнее регуляризуют модель, но могут замедлить обучение.\n",
    "\n",
    "**Зачем нужен Dropout?  Преимущества Dropout:**\n",
    "\n",
    "*   **Регуляризация (Regularization) – Главная цель Dropout:**\n",
    "    *   **Предотвращает переобучение (overfitting):**  Dropout \"ломает\" зависимость нейронов друг от друга.  Нейроны не могут полагаться на конкретные \"соседние\" нейроны, потому что в следующий раз они могут быть \"выключены\".  Это заставляет нейроны **учиться более робастным и независимым признакам**.\n",
    "    *   **Эффект ансамбля (Ensemble effect):**  Каждая конфигурация \"выключенных\" нейронов в мини-батче – это как обучение **немного другой нейронной сети**.  Dropout можно рассматривать как **обучение большого ансамбля моделей**, которые \"разделяют\" веса.  Во время применения (inference) все эти \"подсети\" усредняются, что улучшает обобщающую способность.\n",
    "    *   **Уменьшает co-adaptation нейронов:**  Dropout предотвращает ситуацию, когда нейроны начинают \"сотрудничать\" и специализироваться на определенных признаках обучающих данных слишком сильно.  Он заставляет нейроны учиться **более общим и полезным признакам**.\n",
    "\n",
    "**Где использовать Dropout?**\n",
    "\n",
    "Dropout обычно применяют к **полносвязным (Dense) слоям** и иногда к **сверточным (Conv2D) слоям** в нейронных сетях.  Обычно **после функций активации** в скрытых слоях.  Например:\n",
    "\n",
    "`Activation -> Dropout -> Linear -> ...`\n",
    "\n",
    "**Подведем итог по BatchNorm и Dropout:**\n",
    "\n",
    "*   **BatchNorm:**\n",
    "    *   **Нормализует выходы слоев в мини-батчах.**\n",
    "    *   **Ускоряет обучение, позволяет использовать более высокие learning rates.**\n",
    "    *   **Регуляризация (небольшой эффект).**\n",
    "    *   **Снижает проблемы vanishing/exploding gradients.**\n",
    "    *   Используется **после линейных/сверточных слоев, перед активациями.**\n",
    "\n",
    "*   **Dropout:**\n",
    "    *   **Случайно \"выключает\" нейроны во время обучения.**\n",
    "    *   **Сильная регуляризация, предотвращает переобучение.**\n",
    "    *   **Эффект ансамбля.**\n",
    "    *   Используется **после активаций, обычно в полносвязных слоях.**\n",
    "\n",
    "**Вместе BatchNorm и Dropout – мощный \"дуэт\"** для обучения глубоких нейронных сетей.  BatchNorm помогает ускорить и стабилизировать обучение, а Dropout – регуляризовать модель и улучшить ее обобщающую способность.  В современных нейросетях их часто используют **вместе**, хотя и не всегда.\n",
    "\n",
    "**Вопросы для самопроверки:**\n",
    "\n",
    "1.  В чем основная идея BatchNorm? Что она нормализует и зачем?\n",
    "2.  Какие преимущества дает BatchNorm при обучении нейронных сетей?\n",
    "3.  В чем основная идея Dropout? Как он работает?\n",
    "4.  Какова главная цель Dropout? Какие проблемы он помогает решить?\n",
    "5.  В чем разница в использовании BatchNorm и Dropout во время обучения и применения (inference)?\n",
    "6.  Где обычно располагают слои BatchNorm и Dropout в архитектуре нейронной сети?\n",
    "\n",
    "Готов двигаться дальше и обзорно рассмотреть **архитектуры нейронных сетей (CNN, RNN, LSTM)**? Или есть вопросы по BatchNorm и Dropout?  Не стесняйся спрашивать! 😊  Это важные практические инструменты, которые тебе пригодятся при работе с нейросетями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Супер!  План есть план.  Значит, приступаем к обзору **архитектур нейронных сетей**.  Это как \"типы зданий\", которые можно строить из \"кирпичиков-перцептронов\".  Мы уже хорошо разобрались с основными \"кирпичиками\" (перцептрон, MLP, функции активации, backpropagation, оптимизаторы, BatchNorm, Dropout).  Теперь посмотрим, как из них складывают разные \"здания\" для решения разных задач.\n",
    "\n",
    "### VI.F. Архитектуры нейронных сетей (обзорно) 🏗️: Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM).\n",
    "\n",
    "Мы рассмотрим три основных типа нейронных сетей, которые наиболее часто используются на практике:\n",
    "\n",
    "1.  **Convolutional Neural Networks (CNN) – Сверточные нейронные сети.**  Архитектура, специально разработанная для обработки **изображений и других данных, имеющих пространственную структуру** (например, видео, медицинские изображения, сигналы).\n",
    "\n",
    "2.  **Recurrent Neural Networks (RNN) – Рекуррентные нейронные сети.**  Архитектура, предназначенная для работы с **последовательными данными** (sequence data), где важен порядок элементов.  Примеры: текст, речь, временные ряды, музыка.\n",
    "\n",
    "3.  **Long Short-Term Memory Networks (LSTM) – Сети долгой краткосрочной памяти.**  **Улучшенная версия RNN**, которая лучше справляется с **длинными последовательностями** и проблемой **затухания градиентов** в RNN.\n",
    "\n",
    "**Вспоминаем MLP (Multi-Layer Perceptron):**\n",
    "\n",
    "Прежде чем перейти к CNN и RNN, давай еще раз вспомним **многослойный перцептрон (MLP)**.  MLP – это базовая, \"полносвязная\" нейронная сеть.  В MLP каждый нейрон в слое связан **со всеми** нейронами в предыдущем слое.  MLP хорошо подходит для задач, где входные данные представляют собой **вектор признаков**, и **нет выраженной пространственной или временной структуры**.  Например, классификация табличных данных, предсказание цены дома по набору характеристик и т.п.\n",
    "\n",
    "**Ограничения MLP:**\n",
    "\n",
    "*   **Не учитывает пространственную структуру:**  Для изображений MLP \"не видит\" пространственное расположение пикселей.  Он воспринимает изображение как просто набор чисел, \"размазанных\" в векторе.  Это **неэффективно** для обработки изображений, где важны локальные признаки (границы, текстуры, объекты).\n",
    "*   **Не подходит для последовательностей:**  MLP обрабатывает каждый вход **независимо**.  Он не умеет \"запоминать\" предыдущие входы и использовать эту информацию для обработки текущего входа.  Для последовательных данных (например, текст) это **критически важно**, так как смысл слова часто зависит от контекста – предыдущих слов в предложении.\n",
    "*   **Большое количество параметров:**  В MLP с большим количеством слоев и нейронов количество весов может быть **огромным**.  Это требует **много памяти и вычислительных ресурсов**, и может приводить к **переобучению**, особенно при ограниченном объеме данных.\n",
    "\n",
    "**CNN, RNN и LSTM – это архитектуры, которые \"заточены\" под определенные типы данных и решают проблемы MLP в этих областях.**\n",
    "\n",
    "### VI.F.1. Convolutional Neural Networks (CNNs) 🖼️\n",
    "\n",
    "**Для чего нужны CNN?**  **Для обработки изображений и пространственных данных.**\n",
    "\n",
    "**Основная идея CNN:**  **Свертка (Convolution)** и **Пулинг (Pooling)**.\n",
    "\n",
    "**Свертка (Convolution):**\n",
    "\n",
    "Представь, что у тебя есть изображение (матрица пикселей).  **Сверточный слой** в CNN использует **небольшое \"окно\" (фильтр или ядро свертки)**, которое \"скользит\" по изображению, выполняя операцию **свертки** в каждой позиции.\n",
    "\n",
    "**Что такое свертка?**  Это поэлементное **умножение** значений пикселей в \"окне\" на **веса фильтра** и **суммирование** результатов.  Фильтр – это, по сути, **набор весов**, которые сеть **учит** в процессе обучения.\n",
    "\n",
    "**Интуиция:**  Каждый фильтр учится **детектировать определенный тип признака** на изображении: **границы, углы, текстуры, цветовые пятна** и т.д.  Когда фильтр \"скользит\" по изображению, он \"реагирует\" (выдает большое значение) в тех местах, где есть \"его\" признак.\n",
    "\n",
    "**Результат свертки:**  **Карта признаков (Feature Map)**.  Это новое изображение, которое показывает, где и насколько сильно \"сработал\" данный фильтр на исходном изображении.  CNN может использовать **несколько фильтров** в каждом сверточном слое, каждый из которых учится детектировать свой набор признаков.\n",
    "\n",
    "**Пулинг (Pooling):**\n",
    "\n",
    "**Пулинг слои** используются для **уменьшения пространственного размера** карт признаков, полученных после сверточных слоев.  Обычно используется **Max Pooling** или **Average Pooling**.\n",
    "\n",
    "**Max Pooling:**  В каждом небольшом \"окне\" на карте признаков выбирается **максимальное значение**.  Это значение \"представляет\" все \"окно\" и записывается в выходную карту признаков меньшего размера.\n",
    "\n",
    "**Интуиция:**  Пулинг делает признаки **более устойчивыми к небольшим сдвигам и искажениям** на изображении.  Он также **уменьшает количество параметров** в сети и **ускоряет вычисления**.\n",
    "\n",
    "**Архитектура CNN:**  CNN обычно состоит из **чередования сверточных слоев и пулинг слоев**, за которыми могут следовать **полносвязные слои (MLP)** в конце сети для классификации или других задач.\n",
    "\n",
    "**Типичная структура CNN:**\n",
    "\n",
    "`Input Image -> Conv2D -> ReLU -> Pooling -> Conv2D -> ReLU -> Pooling -> ... -> Flatten -> Dense -> ReLU -> Dropout -> Dense -> Output`\n",
    "\n",
    "*   `Conv2D` – сверточный слой.\n",
    "*   `ReLU` – функция активации.\n",
    "*   `Pooling` – слой пулинга (например, MaxPooling2D).\n",
    "*   `Flatten` – слой, который \"вытягивает\" многомерную карту признаков в одномерный вектор.\n",
    "*   `Dense` – полносвязный слой (MLP).\n",
    "*   `Dropout` – слой Dropout (регуляризация).\n",
    "\n",
    "**Применение CNN:**\n",
    "\n",
    "*   **Классификация изображений (Image Classification):**  Определение, что изображено на картинке (кошка, собака, автомобиль и т.д.).\n",
    "*   **Обнаружение объектов (Object Detection):**  Нахождение и классификация объектов на изображении (например, нахождение всех лиц и автомобилей на фотографии).\n",
    "*   **Сегментация изображений (Image Segmentation):**  Разделение изображения на области, соответствующие разным объектам или классам (например, выделение пикселей, принадлежащих дороге, зданиям, деревьям на спутниковом снимке).\n",
    "*   **Распознавание лиц (Face Recognition), анализ медицинских изображений, компьютерное зрение в робототехнике и самоуправляемых автомобилях** – и многое другое.\n",
    "\n",
    "**Преимущества CNN:**\n",
    "\n",
    "*   **Эффективное извлечение признаков из изображений:**  Сверточные слои автоматически учатся детектировать важные признаки.\n",
    "*   **Учет пространственной структуры:**  Свертка и пулинг сохраняют и используют пространственные отношения между пикселями.\n",
    "*   **Параметрическая эффективность (Parameter Efficiency):**  За счет **параметрического разделения (parameter sharing)** в сверточных слоях (один и тот же фильтр используется во всех позициях изображения) CNN имеют **значительно меньше параметров**, чем MLP для обработки изображений.  Это уменьшает риск переобучения и ускоряет обучение.\n",
    "*   **Инвариантность к сдвигам (Translation Invariance):**  CNN устойчивы к небольшим сдвигам объектов на изображении, благодаря свертке и пулингу.\n",
    "\n",
    "**Интуитивная аналогия:**  Представь, что ты рассматриваешь картину через лупу.  Лупа – это как сверточный фильтр.  Ты водишь лупой по картине, чтобы рассмотреть детали (признаки) в разных местах.  Пулинг – это как уменьшение размера картины, чтобы увидеть ее целиком, но при этом сохранить главные детали.\n",
    "\n",
    "### VI.F.2. Recurrent Neural Networks (RNNs) 🌊\n",
    "\n",
    "**Для чего нужны RNN?**  **Для обработки последовательных данных (текст, речь, временные ряды).**\n",
    "\n",
    "**Основная идея RNN:**  **Рекуррентность (Recurrence) и память.**\n",
    "\n",
    "**Рекуррентность (Recurrence):**\n",
    "\n",
    "RNN обрабатывает последовательность данных **пошагово**, **элемент за элементом**.  Ключевая особенность RNN – **рекуррентная связь**.  Выход RNN на **текущем шаге** (при обработке текущего элемента последовательности) **зависит не только от текущего входа, но и от *предыдущих* выходов (или состояний)**.\n",
    "\n",
    "**Интуиция:**  RNN имеет \"память\" о предыдущих шагах обработки последовательности.  Она использует эту \"память\" для обработки текущего элемента и для формирования выхода.\n",
    "\n",
    "**Развертка RNN во времени (Unrolling RNN):**\n",
    "\n",
    "Чтобы лучше понять, как работает RNN, можно \"развернуть\" ее во времени.  Представь, что у нас есть последовательность входов `x1, x2, x3, ...`.  RNN можно представить как цепочку **одинаковых блоков**, каждый из которых обрабатывает **один элемент последовательности** и передает свое **состояние** следующему блоку.\n",
    "\n",
    "```\n",
    "x1 --> RNN_блок --> h1 ---\\\n",
    "                             \\\n",
    "x2 --> RNN_блок --> h2 -----/  --> Выход y2\n",
    "                             \\\n",
    "x3 --> RNN_блок --> h3 -----/  --> Выход y3\n",
    "       ...\n",
    "```\n",
    "\n",
    "*   `x_t` – вход на шаге `t` (элемент последовательности).\n",
    "*   `RNN_блок` – рекуррентный блок, который содержит веса и функцию активации.  Один и тот же блок используется на каждом шаге.\n",
    "*   `h_t` – **скрытое состояние (hidden state)** RNN на шаге `t`.  Это \"память\" RNN на шаге `t`.  `h_t` зависит от текущего входа `x_t` и **предыдущего состояния** `h_{t-1}`.\n",
    "*   `y_t` – выход RNN на шаге `t`.  Может зависеть от `h_t` (не всегда нужен выход на каждом шаге).\n",
    "\n",
    "**Формулы RNN (упрощенно):**\n",
    "\n",
    "```\n",
    "h_t = activation(W_xh * x_t + W_hh * h_{t-1} + b_h)  // Вычисление скрытого состояния\n",
    "y_t = output_activation(W_hy * h_t + b_y)        // Вычисление выхода (опционально)\n",
    "```\n",
    "\n",
    "*   `W_xh`, `W_hh`, `W_hy`, `b_h`, `b_y` – **веса и смещения RNN**.  Одни и те же веса используются **на всех шагах** обработки последовательности (parameter sharing – как в CNN).\n",
    "*   `activation` и `output_activation` – функции активации (например, tanh, ReLU).\n",
    "\n",
    "**Применение RNN:**\n",
    "\n",
    "*   **Обработка естественного языка (NLP):**\n",
    "    *   **Генерация текста (Text Generation):**  Создание новых текстов, похожих на обучающие данные.\n",
    "    *   **Машинный перевод (Machine Translation):**  Перевод текста с одного языка на другой.\n",
    "    *   **Классификация текста (Text Classification):**  Определение темы, тональности текста и т.д.\n",
    "    *   **Анализ тональности (Sentiment Analysis), чат-боты, распознавание именованных сущностей** – и многое другое.\n",
    "*   **Распознавание речи (Speech Recognition):**  Преобразование аудио в текст.\n",
    "*   **Временные ряды (Time Series Analysis):**  Прогнозирование временных рядов (цен акций, погоды и т.д.).\n",
    "*   **Генерация музыки (Music Generation), анализ ДНК-последовательностей, управление роботами, видеоаналитика** – и многое другое.\n",
    "\n",
    "**Преимущества RNN:**\n",
    "\n",
    "*   **Обработка последовательностей:**  RNN специально разработаны для работы с последовательными данными.\n",
    "*   **Учет контекста:**  Благодаря рекуррентности RNN могут \"запоминать\" предыдущие элементы последовательности и использовать контекст для обработки текущего элемента.\n",
    "*   **Переменная длина последовательности:**  RNN могут обрабатывать последовательности **разной длины** (в отличие от MLP, которые требуют фиксированной длины входа).\n",
    "*   **Параметрическая эффективность:**  За счет **параметрического разделения** (одни и те же веса используются на всех шагах) RNN имеют **относительно небольшое количество параметров**, даже для обработки длинных последовательностей.\n",
    "\n",
    "**Ограничения RNN:**\n",
    "\n",
    "*   **Проблема затухания градиентов (Vanishing Gradients) в длинных последовательностях:**  В \"обычных\" RNN (с функциями активации типа tanh или sigmoid) градиенты могут **экспоненциально затухать** при обратном распространении ошибки через длинные последовательности.  Это затрудняет обучение RNN на длинных последовательностях и \"забывание\" информации из далекого прошлого.\n",
    "*   **Проблема взрывающихся градиентов (Exploding Gradients)** также может возникать в RNN.\n",
    "\n",
    "### VI.F.3. Long Short-Term Memory Networks (LSTM) 🧠\n",
    "\n",
    "**Для чего нужны LSTM?**  **Для обработки длинных последовательностей и решения проблемы затухания градиентов в RNN.**\n",
    "\n",
    "**LSTM – это специальный тип RNN, который был разработан для решения проблемы затухания градиентов в \"обычных\" RNN.**  LSTM гораздо лучше справляются с **длинными последовательностями** и могут **\"запоминать\" информацию на длительных промежутках времени**.\n",
    "\n",
    "**Основная идея LSTM:**  **Ячейка памяти (Memory Cell) и вентили (Gates).**\n",
    "\n",
    "**LSTM ячейка (LSTM Cell):**  В отличие от \"обычного\" RNN-блока, LSTM-блок (LSTM cell) имеет **более сложную внутреннюю структуру**.  Ключевой компонент LSTM – **ячейка памяти (cell state, c_t)**.  Ячейка памяти – это как **\"долговременная память\"** LSTM.  Она может **хранить информацию на протяжении длительных последовательностей**.\n",
    "\n",
    "**Вентили (Gates):**  LSTM использует специальные механизмы, называемые **вентилями (gates)**, для **контроля потока информации** в ячейку памяти и из нее.  В LSTM есть три основных типа вентилей:\n",
    "\n",
    "1.  **Вентиль забывания (Forget Gate):**  Решает, **какую информацию нужно \"забыть\"** из ячейки памяти.\n",
    "2.  **Входной вентиль (Input Gate):**  Решает, **какую новую информацию нужно \"записать\"** в ячейку памяти.\n",
    "3.  **Выходной вентиль (Output Gate):**  Решает, **какую информацию нужно \"выдать\"** из ячейки памяти в качестве выхода LSTM-блока.\n",
    "\n",
    "**Вентили – это, по сути, нейронные сети (обычно сигмоидные слои), которые принимают на вход текущий вход и предыдущее скрытое состояние и выдают значения от 0 до 1.**  Эти значения используются для **\"открытия\" или \"закрытия\"** потока информации.\n",
    "\n",
    "**Формулы LSTM (упрощенно, для понимания идеи):**  (Полные формулы LSTM довольно громоздкие, но главное понять концепцию)\n",
    "\n",
    "*   **Forget Gate:**  `f_t = sigmoid(W_xf * x_t + W_hf * h_{t-1} + b_f)`  (решает, что забыть)\n",
    "*   **Input Gate:**  `i_t = sigmoid(W_xi * x_t + W_hi * h_{t-1} + b_i)`  (решает, что добавить)\n",
    "*   **Cell State Update:**  `c_t = f_t * c_{t-1} + i_t * tanh(W_xc * x_t + W_hc * h_{t-1} + b_c)`  (обновление памяти)\n",
    "*   **Output Gate:**  `o_t = sigmoid(W_xo * x_t + W_ho * h_{t-1} + b_o)`  (решает, что выдать)\n",
    "*   **Hidden State:**  `h_t = o_t * tanh(c_t)`  (выход LSTM-блока)\n",
    "\n",
    "**Интуиция:**  LSTM – это как **совершенная память** с механизмами **записи, чтения и стирания информации**.  Вентили позволяют LSTM **селективно \"запоминать\" важную информацию на длительное время и \"забывать\" неважную информацию**.  Благодаря этому LSTM эффективно справляются с длинными последовательностями и проблемой затухания градиентов.\n",
    "\n",
    "**Применение LSTM:**\n",
    "\n",
    "LSTM используются **там же, где и \"обычные\" RNN, но особенно эффективны для задач, где важна обработка *длинных* последовательностей и удержание *долгосрочных зависимостей*:**\n",
    "\n",
    "*   **Обработка естественного языка (NLP):**  **Машинный перевод, генерация текста, анализ тональности, ответы на вопросы, диалоговые системы** – все задачи NLP, особенно те, где важен контекст на больших расстояниях в тексте.  LSTM – часто \"стандарт\" для многих NLP задач.\n",
    "*   **Распознавание речи (Speech Recognition):**  LSTM хорошо подходят для распознавания речи, особенно в шумных условиях и для длинных аудиозаписей.\n",
    "*   **Временные ряды (Time Series Analysis):**  Прогнозирование финансовых временных рядов, анализ медицинских данных, предсказание трафика и т.д.\n",
    "*   **Управление роботами, анализ видео, генерация музыки** – и другие задачи, где важна память и обработка последовательностей.\n",
    "\n",
    "**Преимущества LSTM:**\n",
    "\n",
    "*   **Эффективная обработка длинных последовательностей:**  LSTM гораздо лучше \"обычных\" RNN справляются с длинными последовательностями.\n",
    "*   **Решение проблемы затухания градиентов:**  Механизмы вентилей в LSTM позволяют градиентам \"беспрепятственно\" распространяться через ячейку памяти на длительные расстояния во времени.\n",
    "*   **Захват долгосрочных зависимостей:**  LSTM могут улавливать и использовать зависимости между элементами последовательности, находящимися далеко друг от друга.\n",
    "*   **Широкое применение в NLP и других областях.**\n",
    "\n",
    "**Разновидности LSTM:**  Существуют разные варианты LSTM (например, GRU – Gated Recurrent Unit, более простая версия LSTM, но тоже очень эффективная).\n",
    "\n",
    "**Подведем итог по архитектурам нейронных сетей (CNN, RNN, LSTM):**\n",
    "\n",
    "*   **MLP (многослойный перцептрон):**  Базовая полносвязная сеть, для векторных данных, не учитывает пространственную или временную структуру.\n",
    "*   **CNN (сверточные нейронные сети):**  Для изображений и пространственных данных.  Используют свертку и пулинг для эффективного извлечения признаков и учета пространственной структуры.\n",
    "*   **RNN (рекуррентные нейронные сети):**  Для последовательных данных.  Используют рекуррентность для обработки последовательностей по шагам и учета контекста.  Могут испытывать проблемы с длинными последовательностями (затухание градиентов).\n",
    "*   **LSTM (сети долгой краткосрочной памяти):**  Улучшенная версия RNN, специально для длинных последовательностей.  Используют ячейку памяти и вентили для решения проблемы затухания градиентов и эффективного запоминания информации на длительное время.\n",
    "\n",
    "**Выбор архитектуры:**  Выбор архитектуры нейронной сети зависит от **типа данных** и **задачи**.\n",
    "\n",
    "*   **Изображения:**  CNN – лучший выбор.\n",
    "*   **Последовательности (текст, речь, временные ряды):**  RNN или LSTM (LSTM часто предпочтительнее для длинных последовательностей).\n",
    "*   **Табличные данные, векторные признаки:**  MLP может быть достаточен, но можно также использовать и другие архитектуры, в зависимости от задачи.\n",
    "\n",
    "**Важно:**  Это был **очень краткий обзор** архитектур.  Каждая из них – это целая область исследований, и мы еще будем углубляться в детали по мере изучения.  Но для начала важно понять **основные идеи и различия** между ними.\n",
    "\n",
    "**Вопросы для самопроверки:**\n",
    "\n",
    "1.  Для каких типов данных лучше всего подходят CNN, RNN и LSTM?\n",
    "2.  В чем основная идея CNN? Какие ключевые операции используются в CNN?\n",
    "3.  В чем основная идея RNN? Что такое рекуррентность и скрытое состояние?\n",
    "4.  В чем основное отличие LSTM от \"обычных\" RNN? Зачем нужны ячейка памяти и вентили?\n",
    "5.  Какая архитектура лучше подойдет для классификации изображений? Для машинного перевода? Для прогнозирования временных рядов?\n",
    "\n",
    "Готов двигаться дальше к последнему пункту в разделе \"Нейронные сети - Введение\" – **методам инициализации весов (Xavier/Glorot, He initialization)**?  Или есть вопросы по архитектурам нейронных сетей?  Не стесняйся спрашивать! 😊  Это обзорно, но важно иметь общее представление о разных типах \"нейросетевых зданий\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.G. Методы инициализации весов: Xavier/Glorot initialization, He initialization ⚖️.\n",
    "\n",
    "**Зачем нужна инициализация весов?**\n",
    "\n",
    "Когда мы создаем нейронную сеть, нам нужно как-то **задать начальные значения весов**.  Казалось бы, можно просто инициализировать все веса **нулями** или **случайными числами**.  Но оказывается, что **способ инициализации весов имеет большое значение**.\n",
    "\n",
    "**Проблема нулевой инициализации:**\n",
    "\n",
    "Если мы инициализируем **все веса нулями**, то **нейронная сеть фактически не будет учиться!**  Почему?\n",
    "\n",
    "*   **Симметрия:**  Все нейроны в каждом слое будут вычислять **одинаковые выходы** и иметь **одинаковые градиенты** при backpropagation.\n",
    "*   **Отсутствие обучения:**  Веса будут обновляться **одинаково**, и нейроны останутся симметричными **на протяжении всего процесса обучения**.  В итоге, нейронная сеть будет вести себя как **линейная модель**, даже если она многослойная и нелинейная.  Она не сможет выучить сложные нелинейные зависимости.\n",
    "\n",
    "**Проблема случайной инициализации (слишком маленькие или слишком большие значения):**\n",
    "\n",
    "Просто случайная инициализация весов (например, из стандартного нормального распределения) тоже может быть неоптимальной, особенно для глубоких сетей.\n",
    "\n",
    "*   **Vanishing/Exploding Gradients (снова!):**  Если веса инициализированы **слишком маленькими**, то при прямом проходе **активации слоев могут становиться все меньше и меньше**, а при обратном проходе **градиенты могут затухать** (vanishing gradients).  Если веса инициализированы **слишком большими**, то **активации и градиенты могут взрываться** (exploding gradients).  Мы уже обсуждали эти проблемы, но теперь видим, что **инициализация весов – одна из причин** их возникновения.\n",
    "*   **Замедление обучения:**  Плохая инициализация может привести к тому, что сеть будет **медленно сходиться** или **застрянет в плохом локальном минимуме**.\n",
    "\n",
    "**Идея хорошей инициализации:**\n",
    "\n",
    "Хорошая инициализация весов должна **обеспечить, чтобы активации нейронов не становились слишком большими и не становились слишком маленькими в начале обучения**.  Нужно поддерживать **примерно одинаковую \"дисперсию\" активаций** на всех слоях сети.  Это помогает **градиентам распространяться эффективно** и **ускоряет обучение**.\n",
    "\n",
    "**Два популярных метода инициализации весов:**\n",
    "\n",
    "1.  **Xavier/Glorot Initialization (Инициализация Ксавье/Глоро):**\n",
    "\n",
    "    *   **Цель:**  Поддерживать **дисперсию активаций примерно одинаковой** на каждом слое в **прямом проходе** (forward pass) и **дисперсию градиентов примерно одинаковой** при **обратном распространении** (backward pass).\n",
    "    *   **Предположения:**  Линейные функции активации или **сигмоида и tanh (в линейной области)**.\n",
    "    *   **Формула (для весов слоя `l`):**  Веса инициализируются **случайными числами из равномерного распределения** в диапазоне:\n",
    "\n",
    "        `[-√(6 / (n_in + n_out)), √(6 / (n_in + n_out))]`\n",
    "\n",
    "        или из **нормального распределения** с **нулевым средним и стандартным отклонением**:\n",
    "\n",
    "        `√(2 / (n_in + n_out))`\n",
    "\n",
    "        где:\n",
    "        *   `n_in` – количество входов (нейронов в предыдущем слое) для текущего слоя.  **Fan-in**.\n",
    "        *   `n_out` – количество выходов (нейронов в текущем слое).  **Fan-out**.\n",
    "\n",
    "    *   **Интуитивное объяснение:**  Xavier initialization **масштабирует веса** в зависимости от **количества входов и выходов** слоя.  Чем **больше входов** и **меньше выходов**, тем **меньше должны быть веса** (чтобы предотвратить взрыв активаций).  И наоборот.  Это помогает **сбалансировать дисперсию активаций** и градиентов.\n",
    "    *   **Когда использовать:**  Xavier/Glorot initialization хорошо подходит для сетей с функциями активации **сигмоида и tanh** (особенно если они работают в линейной области).  Названа в честь Ксавье Глоро, одного из авторов метода.\n",
    "\n",
    "2.  **He Initialization (Инициализация Хе):**\n",
    "\n",
    "    *   **Цель:**  Аналогична Xavier, но **специально для ReLU и ее вариаций**.  Xavier initialization **не оптимальна для ReLU**.\n",
    "    *   **Предположения:**  **ReLU и ее вариации (Leaky ReLU, ELU, GELU и т.д.)**.\n",
    "    *   **Формула (для весов слоя `l`):**  Веса инициализируются **случайными числами из равномерного распределения** в диапазоне:\n",
    "\n",
    "        `[-√(6 / n_in), √(6 / n_in)]`\n",
    "\n",
    "        или из **нормального распределения** с **нулевым средним и стандартным отклонением**:\n",
    "\n",
    "        `√(2 / n_in)`\n",
    "\n",
    "        где:\n",
    "        *   `n_in` – количество входов (нейронов в предыдущем слое) для текущего слоя.  **Fan-in**.\n",
    "\n",
    "    *   **Интуитивное объяснение:**  He initialization **масштабирует веса только в зависимости от количества *входов* (fan-in) слоя**, не учитывая количество выходов (fan-out).  Почему так?  Функция ReLU **обрезает отрицательные значения до нуля**.  Это означает, что **половина нейронов ReLU могут быть \"неактивны\"** (выдавать ноль) в начале обучения.  Чтобы компенсировать это \"обрезание\", He initialization делает веса **немного больше**, чем Xavier, чтобы **увеличить дисперсию активаций** и \"активировать\" больше нейронов ReLU в начале обучения.\n",
    "    *   **Когда использовать:**  He initialization – **лучший выбор для сетей с функциями активации ReLU и ее вариациями**.  Названа в честь Каиминга Хе, одного из авторов метода.\n",
    "\n",
    "**В чем разница между Xavier и He initialization?**\n",
    "\n",
    "*   **Xavier/Glorot:**  Учитывает **fan-in и fan-out**.  Лучше подходит для **сигмоиды и tanh**.\n",
    "*   **He:**  Учитывает **только fan-in**.  Лучше подходит для **ReLU и ее вариаций**.\n",
    "\n",
    "**Практические рекомендации по инициализации весов:**\n",
    "\n",
    "*   **ReLU и ее вариации:**  Используйте **He initialization**.  Это **стандартный и рекомендуемый метод** для ReLU.\n",
    "*   **Sigmoid и Tanh:**  Используйте **Xavier/Glorot initialization**.\n",
    "*   **В современных фреймворках (PyTorch, TensorFlow):**  Методы инициализации весов часто **уже реализованы и используются по умолчанию**.  Например, в PyTorch слои `nn.Linear` и `nn.Conv2d` используют **Kaiming Uniform initialization (вариацию He initialization)** по умолчанию.  Но **важно понимать, какие методы инициализации существуют и когда какой метод лучше использовать**.  Иногда может потребоваться **изменить метод инициализации по умолчанию**, особенно при экспериментах с разными архитектурами и функциями активации.\n",
    "\n",
    "**Подведем итог по инициализации весов:**\n",
    "\n",
    "*   **Правильная инициализация весов – важна для успешного обучения нейронных сетей.**\n",
    "*   **Нулевая инициализация – плохая идея, сеть не будет учиться.**\n",
    "*   **Случайная инициализация без масштабирования – может привести к проблемам vanishing/exploding gradients и замедлить обучение.**\n",
    "*   **Xavier/Glorot initialization – для сигмоиды и tanh.**\n",
    "*   **He initialization – для ReLU и ее вариаций.**\n",
    "*   **В современных фреймворках часто используются хорошие методы инициализации по умолчанию, но понимание принципов важно.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
