{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, давай оптимизируем конспект по Логистической Регрессии, включая бинарный и мультиклассовый случаи.\n",
    "\n",
    "---\n",
    "\n",
    "### XI.A. Логистическая Регрессия: Основы\n",
    "\n",
    "**Назначение:** Логистическая регрессия — это **линейный метод для задач классификации**. В отличие от линейной регрессии (предсказывающей непрерывные значения), логистическая регрессия предсказывает **вероятность** принадлежности объекта к определенному классу.\n",
    "\n",
    "**Проблема Линейной Регрессии для Классификации:**\n",
    "Линейная регрессия ($y = Xw + b$) выдает значения в диапазоне $(-\\infty, +\\infty)$, что не подходит для вероятностей (требуется диапазон [0, 1]).\n",
    "\n",
    "**Решение: Сигмоидальная Функция (Sigmoid / Logistic Function):**\n",
    "Логистическая регрессия использует сигмоиду для \"сжатия\" выхода линейной комбинации в диапазон [0, 1].\n",
    "\n",
    "**Как работает (Бинарный случай):**\n",
    "\n",
    "1.  **Линейная комбинация:** Вычисляется $z = Xw + b$.\n",
    "2.  **Сигмоида:** $z$ пропускается через сигмоиду:\n",
    "    $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "3.  **Выход ($\\hat{y}$):** Результат $\\sigma(z)$ интерпретируется как **вероятность принадлежности к классу \"1\"** ($\\hat{y} = P(y=1|X)$). Вероятность класса \"0\" равна $1 - \\hat{y}$.\n",
    "4.  **Принятие решения:** Используется порог (обычно 0.5):\n",
    "    *   Если $\\hat{y} \\ge 0.5$, предсказывается класс \"1\".\n",
    "    *   Если $\\hat{y} < 0.5$, предсказывается класс \"0\".\n",
    "\n",
    "**Свойства Сигмоиды:**\n",
    "\n",
    "*   **Диапазон:** [0, 1].\n",
    "*   **Форма:** S-образная кривая (значение 0.5 при z=0, стремится к 1 при $z \\to +\\infty$, к 0 при $z \\to -\\infty$).\n",
    "*   **Дифференцируемость:** Важно для градиентного спуска. Производная: $\\sigma'(z) = \\sigma(z) (1 - \\sigma(z))$.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Сигмоидальная функция ---\n",
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# --- Визуализация Сигмоиды ---\n",
    "z_vals = np.linspace(-10, 10, 100)\n",
    "sigmoid_vals = sigmoid(z_vals)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(z_vals, sigmoid_vals, label=\"σ(z) = 1 / (1 + e^(-z))\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"σ(z)\")\n",
    "plt.title(\"Сигмоидальная функция\")\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.1, 1.1) # Немного расширим диапазон Y для наглядности\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='y=0.5') # Горизонталь на 0.5\n",
    "plt.axvline(x=0, color='gray', linestyle='--', label='z=0') # Вертикаль на 0\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### XI.B. Функция Потерь: Log Loss (Бинарная Кросс-Энтропия)\n",
    "\n",
    "**Проблема MSE для Классификации:** MSE плохо подходит для вероятностей (не учитывает диапазон [0, 1], невыпуклость функции потерь при использовании с сигмоидой).\n",
    "\n",
    "**Log Loss (Binary Cross-Entropy):** Стандартная функция потерь для бинарной классификации, измеряющая ошибку предсказания вероятностей.\n",
    "\n",
    "**Формула (для одного объекта):**\n",
    "$$L(y, \\hat{y}) = - [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]$$\n",
    "*   `y`: Истинный класс (0 или 1).\n",
    "*   `ŷ`: Предсказанная вероятность класса 1.\n",
    "*   `log()`: Натуральный логарифм.\n",
    "\n",
    "**Интерпретация:**\n",
    "*   Если `y=1`, $L = -\\log(\\hat{y})$. Штраф растет, когда $\\hat{y} \\to 0$.\n",
    "*   Если `y=0`, $L = -\\log(1-\\hat{y})$. Штраф растет, когда $\\hat{y} \\to 1$.\n",
    "*   Штрафует модель за \"уверенные\" неправильные предсказания сильнее, чем за \"неуверенные\".\n",
    "\n",
    "**Средний Log Loss (по всем N объектам):**\n",
    "$$J(w, b) = - \\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n",
    "Именно эту функцию $J(w, b)$ мы минимизируем при обучении.\n",
    "\n",
    "```python\n",
    "# --- Функция Log Loss ---\n",
    "def calculate_log_loss(y_true, y_predicted_proba):\n",
    "  epsilon = 1e-15 # Для численной стабильности (избегаем log(0))\n",
    "  y_predicted_proba = np.clip(y_predicted_proba, epsilon, 1 - epsilon)\n",
    "  log_loss = - np.mean(y_true * np.log(y_predicted_proba) + (1 - y_true) * np.log(1 - y_predicted_proba))\n",
    "  return log_loss\n",
    "\n",
    "# --- Пример использования ---\n",
    "y_true_example = np.array([1, 0, 1, 0, 1])\n",
    "y_predicted_proba_example = np.array([0.9, 0.1, 0.8, 0.3, 0.7])\n",
    "log_loss_example = calculate_log_loss(y_true_example, y_predicted_proba_example)\n",
    "# print(f\"Пример Log Loss = {log_loss_example:.4f}\") # Около 0.1935\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### XI.C. Обучение: Градиентный Спуск для Log Loss\n",
    "\n",
    "**Цель:** Найти оптимальные `w` и `b`, минимизирующие средний Log Loss $J(w, b)$.\n",
    "\n",
    "**Градиенты Log Loss:** (Удивительно похожи на градиенты MSE для линейной регрессии!)\n",
    "\n",
    "*   **По весам `w`:**\n",
    "    $$\\frac{\\partial J}{\\partial w} = \\frac{1}{N} X^T (\\hat{y} - y)$$\n",
    "    (где $\\hat{y} = \\sigma(Xw+b)$)\n",
    "*   **По смещению `b`:**\n",
    "    $$\\frac{\\partial J}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) = \\text{mean}(\\hat{y} - y)$$\n",
    "\n",
    "**Алгоритм Градиентного Спуска:**\n",
    "\n",
    "1.  Инициализировать `w` и `b` (нулями или малыми случайными числами).\n",
    "2.  Повторять `num_iterations` раз:\n",
    "    *   Вычислить предсказанные вероятности: $\\hat{y} = \\sigma(Xw + b)$.\n",
    "    *   Вычислить градиенты $\\frac{\\partial J}{\\partial w}$ и $\\frac{\\partial J}{\\partial b}$.\n",
    "    *   Обновить параметры:\n",
    "        $w := w - \\eta \\frac{\\partial J}{\\partial w}$\n",
    "        $b := b - \\eta \\frac{\\partial J}{\\partial b}$\n",
    "        (где $\\eta$ - `learning_rate`).\n",
    "\n",
    "```python\n",
    "# --- Функции градиентов ---\n",
    "def calculate_gradient_log_loss_w(X, y_true, y_predicted_proba):\n",
    "  errors = y_predicted_proba - y_true.reshape(-1, 1) # Ensure y_true is column vector if needed\n",
    "  gradient_w = (1 / len(y_true)) * np.dot(X.T, errors)\n",
    "  return gradient_w\n",
    "\n",
    "def calculate_gradient_log_loss_b(y_true, y_predicted_proba):\n",
    "  errors = y_predicted_proba - y_true.reshape(-1, 1) # Ensure y_true is column vector if needed\n",
    "  gradient_b = np.mean(errors)\n",
    "  return gradient_b\n",
    "\n",
    "# --- Реализация градиентного спуска (как в исходном коде) ---\n",
    "# (Генерация данных, инициализация, цикл градиентного спуска,\n",
    "#  вычисление y_predicted_proba_logistic, log_loss, градиентов, обновление w и b)\n",
    "# ... (код из блоков 1, 5 исходника) ...\n",
    "\n",
    "# --- Визуализация процесса обучения (как в исходном коде) ---\n",
    "# (Построение графика log_loss_history)\n",
    "# ... (код из блока 6 исходника) ...\n",
    "\n",
    "# --- Визуализация разделяющей границы (как в исходном коде) ---\n",
    "# (Функция predict_class, генерация сетки, предсказание на сетке, contourf, contour)\n",
    "# ... (код из блока 7 исходника) ...\n",
    "# ВАЖНО: использовать обученные w_logistic, b_logistic в этом блоке!\n",
    "\n",
    "# --- Оценка Accuracy (как в исходном коде) ---\n",
    "# (Предсказание классов на обучающих данных, вычисление accuracy_score)\n",
    "# ... (код из блока 8 исходника) ...\n",
    "\n",
    "# --- Сравнение с sklearn (как в исходном коде) ---\n",
    "# (Обучение sklearn.LogisticRegression, сравнение параметров, Log Loss, Accuracy,\n",
    "#  визуализация двух decision boundaries рядом)\n",
    "# ... (код из блока 9 исходника) ...\n",
    "```\n",
    "\n",
    "*Примечание: Код для градиентного спуска, визуализаций и сравнения оставлен как в исходном варианте для полноты, но может быть сокращен или вынесен в функции для большей компактности реального конспекта.*\n",
    "\n",
    "---\n",
    "\n",
    "### XI.D. Мультиклассовая Логистическая Регрессия\n",
    "\n",
    "**Проблема:** Бинарная логистическая регрессия работает только для двух классов.\n",
    "\n",
    "**Решение:** Расширение на K > 2 классов с использованием функции **Softmax**.\n",
    "\n",
    "**Функция Softmax:**\n",
    "Преобразует вектор \"скоров\" (сырых выходов модели) $z = [z_1, ..., z_K]$ в вектор вероятностей $\\hat{y} = [\\hat{y}_1, ..., \\hat{y}_K]$, где $\\sum \\hat{y}_i = 1$ и $\\hat{y}_i \\in [0, 1]$.\n",
    "\n",
    "**Формула:**\n",
    "$$\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "*   Экспоненты делают скоры положительными и усиливают различия.\n",
    "*   Деление на сумму экспонент нормализует результат в вероятности.\n",
    "\n",
    "**Как работает мультиклассовая логистическая регрессия:**\n",
    "\n",
    "1.  **Линейные комбинации:** Для *каждого класса* $k$ вычисляется свой скор:\n",
    "    $z_k = X w_k + b_k$\n",
    "    (Теперь есть матрица весов $W$ размера `(K, n)` и вектор смещений $b$ размера `(K,)`).\n",
    "2.  **Softmax:** Вектор скоров $z = [z_1, ..., z_K]$ пропускается через Softmax для получения вектора вероятностей $\\hat{y} = \\text{softmax}(z)$.\n",
    "3.  **Предсказание:** Выбирается класс с максимальной вероятностью: $\\text{argmax}_k(\\hat{y}_k)$.\n",
    "\n",
    "**Функция Потерь: Категориальная Кросс-Энтропия**\n",
    "Обобщение Log Loss на K классов.\n",
    "\n",
    "**Формула (для одного объекта с one-hot вектором `y`):**\n",
    "$$L(y, \\hat{y}) = - \\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)$$\n",
    "*   `y = [y_1, ..., y_K]`: One-hot вектор истинного класса (1 для истинного класса, 0 для остальных).\n",
    "*   $\\hat{y} = [\\hat{y}_1, ..., \\hat{y}_K]$: Вектор предсказанных вероятностей (выход Softmax).\n",
    "\n",
    "**Упрощение (если $c$ - индекс истинного класса):**\n",
    "$$L = - \\log(\\hat{y}_c)$$\n",
    "Минимизация этой функции заставляет модель максимизировать вероятность истинного класса.\n",
    "\n",
    "**Средняя Категориальная Кросс-Энтропия (по N объектам):**\n",
    "$$J(W, b) = - \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{K} y_{ij} \\log(\\hat{y}_{ij})$$\n",
    "\n",
    "**Обучение:** Обычно используется готовая реализация (например, `sklearn.linear_model.LogisticRegression` с параметром `multi_class='multinomial'`). Алгоритмы оптимизации (например, `lbfgs`) минимизируют категориальную кросс-энтропию для нахождения оптимальных матриц $W$ и векторов $b$.\n",
    "\n",
    "```python\n",
    "# --- Практика Мультиклассовой Лог. Регрессии с sklearn ---\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Генерация данных (как в исходном коде, 3 класса)\n",
    "# ... (код из блока 1 мультиклассового примера) ...\n",
    "\n",
    "# 2. Обучение модели sklearn\n",
    "# Используем multi_class='multinomial' для Softmax\n",
    "model_sklearn_multiclass = SklearnLogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "model_sklearn_multiclass.fit(X_multiclass, y_multiclass)\n",
    "print(\"\\n--- Мультиклассовая LogisticRegression (sklearn) обучена ---\")\n",
    "# print(\"Параметры w:\\n\", model_sklearn_multiclass.coef_) # Матрица (3, 2)\n",
    "# print(\"Параметры b:\", model_sklearn_multiclass.intercept_) # Вектор (3,)\n",
    "\n",
    "# 3. Предсказание вероятностей и классов (как в исходном коде)\n",
    "# ... (код из блока 3 мультиклассового примера) ...\n",
    "# y_predicted_proba_multiclass_sklearn = model_sklearn_multiclass.predict_proba(X_multiclass)\n",
    "# y_predicted_class_multiclass_sklearn = model_sklearn_multiclass.predict(X_multiclass)\n",
    "# print(\"\\nПримеры предсказанных вероятностей:\\n\", y_predicted_proba_multiclass_sklearn[:3])\n",
    "# print(\"Примеры предсказанных классов:\", y_predicted_class_multiclass_sklearn[:3])\n",
    "\n",
    "\n",
    "# 4. Оценка качества: Accuracy и Confusion Matrix (как в исходном коде)\n",
    "# ... (код из блока 4 мультиклассового примера) ...\n",
    "# accuracy_multiclass = accuracy_score(y_multiclass, y_predicted_class_multiclass_sklearn)\n",
    "# cm_multiclass = confusion_matrix(y_multiclass, y_predicted_class_multiclass_sklearn)\n",
    "# print(f\"\\nAccuracy (мультикласс): {accuracy_multiclass:.4f}\")\n",
    "# (код для визуализации confusion matrix с seaborn)\n",
    "# plt.show() # Показать все графики в конце\n",
    "```\n",
    "\n",
    "---\n",
    "Этот вариант объединяет теорию и практику, убирает избыточность, но сохраняет ключевые формулы, объяснения и код для демонстрации. Графики и сравнение с `sklearn` оставлены как важные элементы для понимания."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
