{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Линейная Регрессия\n",
    "\n",
    "#### I.A. Основная Идея и Уравнение\n",
    "\n",
    "**Назначение:** Линейная регрессия — модель для предсказания **непрерывной** целевой переменной ($y$) на основе одного или нескольких признаков ($x$). Модель предполагает **линейную зависимость** между признаками и целью.\n",
    "\n",
    "**Уравнение:**\n",
    "\n",
    "*   **Простая Линейная Регрессия (1 признак):**\n",
    "    $$y = w x + b$$\n",
    "    *   $y$: Целевая переменная.\n",
    "    *   $x$: Признак.\n",
    "    *   $w$: Вес (наклон прямой) - показывает изменение $y$ при изменении $x$ на 1.\n",
    "    *   $b$: Смещение (свободный член) - значение $y$ при $x=0$.\n",
    "\n",
    "*   **Множественная Линейная Регрессия (n признаков):**\n",
    "    $$y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b = \\sum_{j=1}^{n} w_j x_j + b$$\n",
    "    *   $x_j$: j-й признак.\n",
    "    *   $w_j$: Вес j-го признака.\n",
    "\n",
    "*   **Матричная форма (для m объектов и n признаков):**\n",
    "    $$\\hat{y} = X w + b$$\n",
    "    *   $\\hat{y}$: Вектор предсказанных значений (размер $m \\times 1$).\n",
    "    *   $X$: Матрица признаков (размер $m \\times n$). Каждая строка - объект, каждый столбец - признак.\n",
    "    *   $w$: Вектор весов (размер $n \\times 1$).\n",
    "    *   $b$: Смещение (скаляр, часто добавляется как фиктивный признак к $X$, или обрабатывается отдельно).\n",
    "    *(Примечание: Иногда $b$ включают в вектор $w$, добавляя к $X$ столбец из единиц).*\n",
    "\n",
    "**Задача:** Найти оптимальные параметры $w$ (вектор весов) и $b$ (смещение), чтобы модель наилучшим образом описывала данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.B. Функция Ошибки: MSE (Mean Squared Error)\n",
    "\n",
    "**Назначение:** Оценить, насколько хорошо модель (линия/гиперплоскость) описывает данные. Минимизация MSE — цель обучения.\n",
    "\n",
    "**Формула:**\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "*   $N$: Количество объектов.\n",
    "*   $y_i$: Истинное значение для i-го объекта.\n",
    "*   $\\hat{y}_i$: Предсказанное значение для i-го объекта моделью ($w x_i + b$ или $X_i w + b$).\n",
    "*   $(y_i - \\hat{y}_i)$: Ошибка (остаток) для i-го объекта.\n",
    "\n",
    "**Свойства:**\n",
    "*   Штрафует за большие ошибки сильнее из-за возведения в квадрат.\n",
    "*   Всегда неотрицательна (MSE ≥ 0).\n",
    "*   Для линейной регрессии функция MSE является **выпуклой**, что гарантирует существование единственного глобального минимума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.C. Методы Обучения (Поиск Оптимальных Параметров)\n",
    "\n",
    "**1. Метод Наименьших Квадратов (МНК / OLS - Ordinary Least Squares):**\n",
    "*   **Тип:** Аналитический (точное решение).\n",
    "*   **Идея:** Найти $w$ и $b$, минимизирующие сумму квадратов остатков $\\sum (y_i - \\hat{y}_i)^2$.\n",
    "*   **Результат:** Существуют точные формулы для $w$ и $b$ (особенно простая для простой регрессии, для множественной используется матричная алгебра):\n",
    "    *   **Простая ЛР:**\n",
    "        $$w = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}$$\n",
    "        $$b = \\bar{y} - w \\bar{x}$$\n",
    "    *   **Множественная ЛР (матричная форма):**\n",
    "        $$w = (X^T X)^{-1} X^T y$$\n",
    "        *(Требуется обратимость матрицы $X^T X$, что не всегда выполняется, например, при мультиколлинеарности).*\n",
    "*   **Преимущества:** Точное, быстрое для небольшого/среднего числа признаков.\n",
    "*   **Недостатки:** Вычислительно сложен (обращение матрицы) при очень большом числе признаков. Не применим для многих других моделей.\n",
    "\n",
    "**2. Градиентный Спуск (Gradient Descent - GD):**\n",
    "*   **Тип:** Итеративный (приближенное решение).\n",
    "*   **Идея:** Постепенно изменять $w$ и $b$ в направлении **антиградиента** функции ошибки MSE, пока не будет достигнут минимум.\n",
    "*   **Аналогия:** Спуск с горы в тумане, двигаясь в сторону самого крутого уклона вниз.\n",
    "*   **Градиенты MSE:**\n",
    "    *   По весу $w_j$: $\\frac{\\partial MSE}{\\partial w_j} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_{ij}$\n",
    "    *   По смещению $b$: $\\frac{\\partial MSE}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)$\n",
    "    *   **Матричная форма:**\n",
    "        $\\nabla_w MSE = \\frac{2}{N} X^T (\\hat{y} - y)$\n",
    "        $\\nabla_b MSE = \\frac{2}{N} \\sum (\\hat{y} - y)$\n",
    "*   **Шаг обновления:**\n",
    "    $w := w - \\eta \\nabla_w MSE$\n",
    "    $b := b - \\eta \\nabla_b MSE$\n",
    "    *   $\\eta$ (`learning_rate`): Скорость обучения - размер шага. **Важный гиперпараметр!**\n",
    "        *   Маленькая $\\eta$: Медленная сходимость.\n",
    "        *   Большая $\\eta$: Может \"перепрыгнуть\" минимум, расходимость.\n",
    "*   **Преимущества:** Универсален (применим ко многим моделям и функциям потерь), хорошо работает с большим числом признаков.\n",
    "*   **Недостатки:** Требует выбора `learning_rate`, может сходиться медленнее МНК, может застрять в локальном минимуме (но не для MSE линейной регрессии).\n",
    "\n",
    "**(Код для реализации GD, визуализации MSE и линии регрессии, а также использования `sklearn.linear_model.LinearRegression` и сравнения результатов остается как в исходном варианте, он хорошо иллюстрирует процесс).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример использования sklearn (кратко)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# ... (генерация X множественной регрессии, y) ...\n",
    "\n",
    "# Важно: X должен быть 2D массивом для sklearn\n",
    "# if X.ndim == 1:\n",
    "#     X = X.reshape(-1, 1) # Пример для простой ЛР\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "sklearn_w = model.coef_\n",
    "sklearn_b = model.intercept_\n",
    "# print(f'sklearn w: {sklearn_w}, sklearn b: {sklearn_b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### I.D. Регуляризация Линейной Регрессии\n",
    "\n",
    "**Проблема:** Переобучение (Overfitting) - модель слишком хорошо подстраивается под обучающие данные (включая шум) и плохо обобщает на новые. В линейных моделях часто проявляется в **слишком больших значениях весов `w`**.\n",
    "\n",
    "**Регуляризация:** Методы добавления **штрафа** к функции ошибки за большие веса, заставляя модель быть \"проще\" и устойчивее.\n",
    "\n",
    "**1. L2-Регуляризация (Ridge / Гребневая регрессия):**\n",
    "*   **Штраф:** Сумма **квадратов** весов: $\\lambda \\sum w_j^2$.\n",
    "*   **Функция ошибки:** $J_{Ridge} = MSE + \\lambda ||w||_2^2 = \\frac{1}{N} \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum w_j^2$.\n",
    "*   $\\lambda$ (`alpha` в sklearn): Коэффициент регуляризации (гиперпараметр). $\\lambda \\ge 0$.\n",
    "    *   $\\lambda=0$: Обычная ЛР.\n",
    "    *   $\\lambda \\to \\infty$: Все $w_j \\to 0$.\n",
    "*   **Эффект:** \"Стягивает\" веса к нулю, но **не обнуляет** их полностью. Уменьшает влияние всех признаков, особенно коррелированных. Уменьшает дисперсию (variance).\n",
    "\n",
    "**2. L1-Регуляризация (Lasso / Лассо):**\n",
    "*   **Штраф:** Сумма **модулей** весов: $\\lambda \\sum |w_j|$.\n",
    "*   **Функция ошибки:** $J_{Lasso} = MSE + \\lambda ||w||_1 = \\frac{1}{N} \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum |w_j|$.\n",
    "*   $\\lambda$ (`alpha` в sklearn): Коэффициент регуляризации.\n",
    "*   **Эффект:** Может **обнулять** некоторые веса полностью. Выполняет **автоматический отбор признаков**. Делает модель разреженной (sparse). Также уменьшает дисперсию.\n",
    "\n",
    "**3. Elastic Net:**\n",
    "*   **Штраф:** Комбинация L1 и L2 штрафов.\n",
    "*   **Функция ошибки:** $J_{ElasticNet} = MSE + \\lambda_1 ||w||_1 + \\lambda_2 ||w||_2^2$.\n",
    "*   Сочетает преимущества Lasso (отбор признаков) и Ridge (стабильность при коррелированных признаках).\n",
    "\n",
    "**Выбор типа и силы регуляризации (`alpha`/`lambda`):** Обычно с помощью **кросс-валидации**.\n",
    "\n",
    "**(Код для использования `Ridge` и `Lasso` из `sklearn` и сравнения весов остается как в исходном варианте).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ... (генерация X, y) ...\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ridge_model \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;66;03m# alpha = lambda\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m ridge_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m, y)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(\"Ridge weights:\", ridge_model.coef_)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m lasso_model \u001b[38;5;241m=\u001b[39m Lasso(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;66;03m# alpha = lambda\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Пример использования Ridge и Lasso (кратко)\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# ... (генерация X, y) ...\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0) # alpha = lambda\n",
    "ridge_model.fit(X, y)\n",
    "# print(\"Ridge weights:\", ridge_model.coef_)\n",
    "\n",
    "lasso_model = Lasso(alpha=0.1) # alpha = lambda\n",
    "lasso_model.fit(X, y)\n",
    "# print(\"Lasso weights:\", lasso_model.coef_) # Некоторые веса могут быть близки к 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.E. Компромисс Смещения и Дисперсии (Bias-Variance Tradeoff) и Регуляризация\n",
    "\n",
    "*   **Смещение (Bias):** Ошибка из-за упрощающих предположений модели (недообучение).\n",
    "*   **Дисперсия (Variance):** Ошибка из-за чувствительности модели к шуму в обучающих данных (переобучение).\n",
    "\n",
    "**Регуляризация в основном борется с высокой дисперсией (переобучением):**\n",
    "*   Штрафуя большие веса, она **упрощает** модель.\n",
    "*   Упрощенная модель **менее чувствительна** к шуму в обучении.\n",
    "*   **НО:** Чрезмерная регуляризация (слишком большое $\\lambda$) может **увеличить смещение**, сделав модель слишком простой (недообучение).\n",
    "\n",
    "**Цель:** Найти $\\lambda$, обеспечивающую оптимальный баланс между смещением и дисперсией для минимизации ошибки на новых данных.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
