{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный лес (Random Forest)\n",
    "\n",
    "**Ансамблевые методы** — это техники, которые объединяют прогнозы нескольких базовых моделей (например, деревьев решений) для получения более точного и стабильного прогноза.\n",
    "\n",
    "**Почему ансамбли работают лучше?**\n",
    "\n",
    "Основная причина, по которой ансамбли часто превосходят одиночные модели, — это **уменьшение дисперсии (Variance)** и повышение **устойчивости** модели.\n",
    "\n",
    "*   **Уменьшение дисперсии:**  Разные модели, обученные на разных подмножествах данных или с разными параметрами, могут делать разные ошибки.  Когда мы усредняем или голосуем за прогнозы нескольких моделей, ошибки разных моделей могут **компенсировать друг друга**, и общий прогноз становится более стабильным и точным.\n",
    "*   **Повышение устойчивости:**  Ансамбли менее чувствительны к случайным колебаниям в обучающих данных.  Если немного изменить обучающие данные, одиночное дерево решений может сильно измениться.  А ансамбль из множества деревьев будет меняться меньше, что делает его более надежным.\n",
    "\n",
    "**Случайный лес (Random Forest)** — один из самых популярных и мощных ансамблевых методов, особенно хорошо работающий с деревьями решений.\n",
    "\n",
    "**Идея Случайного леса очень проста и элегантна:**\n",
    "\n",
    "1.  **Создаем \"лес\" из множества деревьев решений.**  Обычно это сотни или даже тысячи деревьев.\n",
    "2.  **Каждое дерево обучается на случайной подвыборке обучающих данных.** Это называется **bootstrap aggregating** или **bagging**.  Представь, что мы берем наш обучающий набор данных и случайно выбираем из него, например, 80% объектов **с возвращением** (то есть, один и тот же объект может попасть в подвыборку несколько раз, а какие-то объекты могут не попасть вообще).  На каждой такой случайной подвыборке мы обучаем отдельное дерево решений.\n",
    "3.  **При построении каждого дерева, при выборе признака для разделения в каждом узле, мы тоже делаем случайный выбор.**  Вместо того, чтобы рассматривать все признаки и выбирать лучший, мы случайно выбираем **несколько признаков** (например, корень квадратный из общего числа признаков) и ищем лучший признак для разделения только среди этих случайно выбранных.  Это называется **случайное подпространство признаков** (random feature subspace).\n",
    "4.  **Для классификации:** Чтобы сделать прогноз для нового объекта, мы пропускаем его через **все деревья** в лесу и смотрим, какой класс **большинство деревьев** предсказало.  Это называется **голосование большинством** (majority voting).\n",
    "5.  **Для регрессии:**  Прогнозы всех деревьев **усредняются**.\n",
    "\n",
    "**Ключевые идеи Случайного леса, которые делают его таким эффективным:**\n",
    "\n",
    "*   **Bagging (Bootstrap Aggregating):**  Обучение на случайных подвыборках данных делает деревья **разнообразными**.  Каждое дерево видит немного \"другую\" версию обучающих данных.  Это уменьшает корреляцию между деревьями и снижает дисперсию ансамбля.\n",
    "*   **Случайное подпространство признаков:**  Случайный выбор признаков при каждом разделении еще больше **декоррелирует деревья**.  Если бы мы всегда использовали лучшие признаки для разделения, деревья получались бы более похожими друг на друга, и эффект ансамбля был бы меньше.  Случайный выбор признаков как бы \"заставляет\" каждое дерево фокусироваться на разных аспектах данных.\n",
    "\n",
    "**Преимущества Случайного леса:**\n",
    "\n",
    "*   **Высокая точность и хорошее обобщение:**  Случайный лес часто показывает очень хорошее качество на практике и хорошо обобщает на новые данные.\n",
    "*   **Устойчивость к переобучению:**  Благодаря bagging и случайному выбору признаков, случайный лес гораздо менее склонен к переобучению, чем одиночные деревья решений.  Обычно не требует такой тщательной настройки параметров регуляризации, как отдельные деревья.\n",
    "*   **Оценка важности признаков (Feature Importance):**  Случайный лес позволяет оценивать, какие признаки были наиболее важны для предсказаний.\n",
    "*   **Работает с большими наборами данных и большим числом признаков:**  Случайный лес достаточно эффективно работает даже с данными высокой размерности.\n",
    "*   **Не требует масштабирования признаков:**  Как и деревья решений, случайный лес не чувствителен к масштабу признаков.\n",
    "\n",
    "**Недостатки Случайного леса:**\n",
    "\n",
    "*   **Менее интерпретируем, чем одиночное дерево:**  Случайный лес — это \"черный ящик\" в большей степени, чем одно дерево.  Сложнее понять, почему именно случайный лес принял то или иное решение.\n",
    "*   **Может быть медленнее, чем одиночное дерево:**  Обучение и предсказание случайного леса требует времени, пропорционального количеству деревьев в лесу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность на тестовой выборке: 0.9867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.5)\n",
    "\n",
    "rf_clf_default = RandomForestClassifier(random_state=42)\n",
    "rf_clf_default.fit(X_train, y_train)\n",
    "\n",
    "y_test_def_pred = rf_clf_default.predict(X_test)\n",
    "test_accuracy_rf_pred = accuracy_score(y_test, y_test_def_pred)\n",
    "\n",
    "print(f'Точность на тестовой выборке: {test_accuracy_rf_pred:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты с параметром n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: [5, 10, 20, 50, 100, 10000], Test Accuracy: 0.9733\n",
      "n_estimators: [5, 10, 20, 50, 100, 10000], Test Accuracy: 0.9867\n",
      "n_estimators: [5, 10, 20, 50, 100, 10000], Test Accuracy: 0.9867\n",
      "n_estimators: [5, 10, 20, 50, 100, 10000], Test Accuracy: 0.9867\n",
      "n_estimators: [5, 10, 20, 50, 100, 10000], Test Accuracy: 0.9733\n",
      "n_estimators: [5, 10, 20, 50, 100, 10000], Test Accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [5, 10, 20, 50, 100]\n",
    "\n",
    "for estimator in n_estimators:\n",
    "    rf_clf_estimators = RandomForestClassifier(n_estimators=estimator)\n",
    "    rf_clf_estimators.fit(X_train, y_train)\n",
    "    y_test_pred = rf_clf_estimators.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f'n_estimators: {n_estimators}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Эксперименты с параметром max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth (RF): 2, Test Accuracy: 0.9867\n",
      "Max depth (RF): 3, Test Accuracy: 0.9867\n",
      "Max depth (RF): 4, Test Accuracy: 0.9867\n",
      "Max depth (RF): 5, Test Accuracy: 0.9867\n",
      "Max depth (RF): 6, Test Accuracy: 0.9867\n",
      "Max depth (RF): 7, Test Accuracy: 0.9867\n",
      "Max depth (RF): 8, Test Accuracy: 0.9867\n",
      "Max depth (RF): None, Test Accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "max_depths_rf = [2, 3, 4, 5, 6, 7, 8, None] # Разные значения max_depth\n",
    "\n",
    "for depth in max_depths_rf:\n",
    "    rf_clf_depth_limited = RandomForestClassifier(max_depth=depth, n_estimators=100, random_state=42) # Задаем max_depth и фиксируем n_estimators=100\n",
    "    rf_clf_depth_limited.fit(X_train, y_train)\n",
    "    y_test_pred_rf_depth = rf_clf_depth_limited.predict(X_test)\n",
    "    test_accuracy_rf_depth = accuracy_score(y_test, y_test_pred_rf_depth)\n",
    "\n",
    "    print(f'Max depth (RF): {depth}, Test Accuracy: {test_accuracy_rf_depth:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка важности признаков (Feature Importance) в Случайном Лесу 🌟\n",
    "\n",
    "Одно из полезных свойств случайного леса — возможность оценивать важность признаков. Случайный лес может сказать нам, какие признаки в данных были наиболее важны для классификации (или регрессии).\n",
    "\n",
    "В scikit-learn у обученного RandomForestClassifier есть атрибут feature_importances_, который содержит оценки важности признаков. Давай посмотрим на них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Importance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a3dbbe58-467b-4e45-90d7-2c6a8630381e",
       "rows": [
        [
         "2",
         "petal length (cm)",
         "0.45605861445772294"
        ],
        [
         "3",
         "petal width (cm)",
         "0.395960486579044"
        ],
        [
         "0",
         "sepal length (cm)",
         "0.11242554341258987"
        ],
        [
         "1",
         "sepal width (cm)",
         "0.035555355550643084"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>0.456059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.395960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.112426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.035555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "2  petal length (cm)    0.456059\n",
       "3   petal width (cm)    0.395960\n",
       "0  sepal length (cm)    0.112426\n",
       "1   sepal width (cm)    0.035555"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = rf_clf_default.feature_importances_\n",
    "import_df = pd.DataFrame({'Feature': iris.feature_names, 'Importance' : feature_importances})\n",
    "import_df = import_df.sort_values(by='Importance', ascending=False)\n",
    "display(import_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краткое резюме по Случайному Лесу:\n",
    "\n",
    "Случайный лес — это мощный ансамблевый метод, который строит \"лес\" из множества деревьев решений.\n",
    "\n",
    "Bagging (обучение на случайных подвыборках) и случайный выбор признаков делают деревья в лесу разнообразными и уменьшают корреляцию между ними.\n",
    "\n",
    "Случайный лес обычно показывает более высокое качество и лучшую устойчивость к переобучению, чем одиночные деревья.\n",
    "\n",
    "Параметр n_estimators (количество деревьев) обычно влияет на качество: чем больше деревьев, тем лучше (до определенного предела).\n",
    "\n",
    "max_depth и другие параметры регуляризации также важны, но случайный лес менее чувствителен к переобучению из-за глубины деревьев, чем одиночные деревья.\n",
    "\n",
    "Случайный лес предоставляет оценку важности признаков, что полезно для понимания данных и отбора признаков."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "konspect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
