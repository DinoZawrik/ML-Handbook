![Logo](Logo.jpeg)

# Основы Машинного Обучения: Конспекты и Примеры

Этот репозиторий содержит Jupyter ноутбуки с конспектами и практическими примерами по основным темам и алгоритмам машинного обучения.

## Содержание

*   [**1. NumPy** (`1_NumPy.ipynb`)](#1-numpy) - Основы NumPy: массивы, операции, индексация, бродкастинг, ufuncs, случайные числа.
*   [**2. Pandas** (`2_Pandas.ipynb`)](#2-pandas) - Основы Pandas: DataFrame, Series, создание, чтение/запись, операции, индексация, фильтрация, изменение данных, `.apply()`, сортировка.
*   [**3. Визуализация данных (Matplotlib & Seaborn)** (`3_Matplotlib_Seaborn.ipynb`)](#3-визуализация-данных-matplotlib--seaborn) - Построение графиков с Matplotlib (plot, scatter, bar, hist) и Seaborn (scatterplot, boxplot, histplot, violinplot, pairplot, heatmap).
*   [**4. Метрики ML** (`4_Metrics.ipynb`)](#4-метрики-ml) - Метрики регрессии (MSE, MAE, MAPE) и классификации (Accuracy, Confusion Matrix, Precision, Recall, F1, ROC AUC, PR AUC), Bias-Variance Tradeoff.
*   [**5.1. Линейная Регрессия** (`5_1_Lineage_model_regress.ipynb`)](#51-линейная-регрессия) - Уравнение, MSE, МНК, Градиентный Спуск, множественная регрессия, регуляризация (Ridge, Lasso).
*   [**5.2. Логистическая Регрессия** (`5_2_Lineage_model_log.ipynb`)](#52-логистическая-регрессия) - Сигмоида, Log Loss, Градиентный Спуск, бинарная и мультиклассовая (Softmax, Categorical Cross-Entropy) классификация.
*   [**6.1. Дерево Решений** (`6_1_Decision_Tree.ipynb`)](#61-дерево-решений) - Структура, построение, критерии разделения (Gini, Entropy), переобучение, регуляризация.
*   [**6.2. Случайный Лес** (`6_2_Random_Forest.ipynb`)](#62-случайный-лес) - Ансамбли, бэггинг, случайные подпространства, важность признаков.
*   [**6.3. Градиентный Бустинг** (`6_3_Gradient_Boosting.ipynb`)](#63-градиентный-бустинг) - Идея бустинга, обучение на ошибках, learning rate, XGBoost, LightGBM, CatBoost.
*   [**7. Метод Опорных Векторов (SVM)** (`7_SVM.ipynb`)](#7-метод-опорных-векторов-svm) - Линейный SVM, зазор, опорные векторы, Hinge Loss, Soft Margin, ядра (Kernels), двойственная задача.
*   [**8. Искусственный Интеллект (Введение)** (`8_AI.ipynb`)](#8-искусственный-интеллект-введение) - (Предположительно) Общие концепции ИИ, типы, области применения.

---

## Детальное Описание Ноутбуков

### 1. NumPy
Файл: [`1_NumPy.ipynb`](1_NumPy.ipynb)

Основы библиотеки NumPy для научных вычислений в Python. Рассматриваются создание `ndarray`, основные атрибуты (`dtype`, `ndim`, `shape`), поэлементные операции, операции с векторами и матрицами (скалярное произведение, матричное умножение, транспонирование), индексация и срезы, изменение формы (`reshape`), бродкастинг, универсальные функции (ufuncs) и генерация случайных чисел (`numpy.random`).

### 2. Pandas
Файл: [`2_Pandas.ipynb`](2_Pandas.ipynb)

Введение в библиотеку Pandas для анализа и манипулирования данными. Создание DataFrame из различных источников (словари, списки, NumPy), основные операции (`head`, `tail`, `info`, `describe`), выбор столбцов и строк (`[]`, `.loc`, `.iloc`), фильтрация данных (логическая индексация, `.isin()`), добавление и изменение столбцов (`=`, `.assign()`, `.rename()`), применение функций к данным (`.apply()`, lambda-функции), сортировка (`.sort_values()`, `.sort_index()`). Кратко затронуты чтение/запись файлов и другие важные операции (groupby, merge, пропуски, astype).

### 3. Визуализация данных (Matplotlib & Seaborn)
Файл: [`3_Matplotlib_Seaborn.ipynb`](3_Matplotlib_Seaborn.ipynb)

Основы визуализации данных. Использование Matplotlib для создания базовых графиков (линейный, точечный, столбчатый, гистограмма) и их настройки. Применение Seaborn для построения более сложных и привлекательных статистических графиков (scatterplot, boxplot, histplot, violinplot, pairplot, heatmap) с интеграцией Pandas DataFrame и кодированием дополнительных измерений (`hue`, `size`, `style`).

### 4. Метрики ML
Файл: [`4_Metrics.ipynb`](4_Metrics.ipynb)

Обзор основных метрик для оценки качества моделей машинного обучения. Рассматриваются метрики для задач регрессии (MSE, MAE, MAPE) и классификации (Accuracy, Confusion Matrix, Precision, Recall, F1-score, ROC AUC, PR AUC). Также обсуждается фундаментальная концепция компромисса смещения и дисперсии (Bias-Variance Tradeoff) и проблемы недообучения/переобучения.

### 5.1. Линейная Регрессия
Файл: [`5_1_Lineage_model_regress.ipynb`](5_1_Lineage_model_regress.ipynb)

Теория и практика линейной регрессии. Уравнение простой и множественной регрессии, матричная запись. Функция ошибки MSE. Методы обучения: аналитический (МНК) и итеративный (Градиентный Спуск). Реализация градиентного спуска с нуля и сравнение с `sklearn.linear_model.LinearRegression`. Регуляризация для борьбы с переобучением: L2 (Ridge) и L1 (Lasso).

### 5.2. Логистическая Регрессия
Файл: [`5_2_Lineage_model_log.ipynb`](5_2_Lineage_model_log.ipynb)

Линейный метод для задач классификации. Сигмоидальная функция для получения вероятностей в бинарном случае. Функция потерь Log Loss (Бинарная Кросс-Энтропия). Обучение с помощью Градиентного Спуска (реализация с нуля и сравнение с `sklearn`). Мультиклассовая логистическая регрессия: функция Softmax, Категориальная Кросс-Энтропия, использование `sklearn.linear_model.LogisticRegression` для мультикласса.

### 6.1. Дерево Решений
Файл: [`6_1_Decision_Tree.ipynb`](6_1_Decision_Tree.ipynb)

Деревья решений как интуитивно понятный метод классификации и регрессии. Структура дерева (узлы, ветви, листья). Процесс построения (рекурсивное разделение). Критерии разделения: Gini impurity, Энтропия, Information Gain (для классификации), MSE (для регрессии). Проблема переобучения и методы регуляризации (ограничение глубины, минимальное число объектов в узле/листе, прюнинг).

### 6.2. Случайный Лес
Файл: [`6_2_Random_Forest.ipynb`](6_2_Random_Forest.ipynb)

Ансамблевый метод на основе деревьев решений. Принцип работы: бэггинг (bootstrap aggregating) и метод случайных подпространств. Преимущества: высокая точность, устойчивость к переобучению. Недостатки: меньшая интерпретируемость. Основные гиперпараметры (`n_estimators`, `max_depth`). Оценка важности признаков (`feature_importances_`). Практика с `sklearn.ensemble.RandomForestClassifier`.

### 6.3. Градиентный Бустинг
Файл: [`6_3_Gradient_Boosting.ipynb`](6_3_Gradient_Boosting.ipynb)

Еще один мощный ансамблевый метод. Идея последовательного построения моделей, где каждая следующая исправляет ошибки предыдущих (бустинг). Обучение на псевдоостатках (градиентах функции потерь). Роль скорости обучения (`learning_rate`). Ключевые гиперпараметры. Обзор популярных и эффективных библиотек: XGBoost, LightGBM, CatBoost, их особенности и сравнение. Практические примеры использования библиотек.

### 7. Метод Опорных Векторов (SVM)
Файл: [`7_SVM.ipynb`](7_SVM.ipynb)

Метод для классификации (и регрессии), основанный на поиске оптимальной разделяющей гиперплоскости с максимальным зазором. Линейный SVM, опорные векторы. Функция потерь Hinge Loss. Soft Margin SVM для нелинейно разделимых данных, параметр регуляризации C. Ядерный трюк (Kernel Trick) для нелинейной классификации: ядра RBF, полиномиальное и др. Двойственная задача SVM и ее связь с ядрами. Практика с `sklearn.svm.SVC`.

### 8. Искусственный Интеллект (Введение)
Файл: [`8_AI.ipynb`](8_AI.ipynb)

*Примечание: Содержание этого ноутбука не было предоставлено для оптимизации, описание является предположением.*
Предположительно, этот ноутбук содержит введение в общие концепции искусственного интеллекта (ИИ), возможно, затрагивая его историю, основные направления (Машинное обучение, Глубокое обучение, NLP, Компьютерное зрение), типы ИИ (слабый/сильный) и различные области применения.

---
